{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sympy\n",
    "import mpmath\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import functools\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "#import separation_algorithm as sepa\n",
    "#import functions_eye_tracker_project as funcs\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import copy\n",
    "import emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10afb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_square(l_emp_points,l_emp_fit):\n",
    "    l_num = np.mean((np.array(l_emp_points)-np.array(l_emp_fit))**2)\n",
    "    l_den = np.std(np.array(l_emp_points))**2\n",
    "    return(1 - l_num/l_den)\n",
    "\n",
    "\n",
    "def adjusted_r_square(l_emp_points,l_emp_fit,degrees_freedom):\n",
    "    n = len(l_emp_points)\n",
    "    l_num = np.mean((np.array(l_emp_points)-np.array(l_emp_fit))**2)\n",
    "    l_den = np.std(np.array(l_emp_points))**2\n",
    "    rsqu = 1 - l_num/l_den\n",
    "    return( 1 - (1 -rsqu)*( (n-1)/(n-degrees_freedom)  ))\n",
    "    \n",
    "    \n",
    "    \n",
    "def powerl_fit(l_tau,l_k,l_a):\n",
    "    return(l_k*np.power(2,l_tau*l_a))\n",
    "\n",
    "def Expr_variable_change(l_alpha,l_beta,D,v):\n",
    "    l_lambdaD = l_beta(1-l_alpha)\n",
    "    l_lambdaB = l_alpha*l_beta\n",
    "    return(l_lambdaD,l_lambdaB,D,v)\n",
    "\n",
    "def Expr_variable_change2(l_lambdaD,l_lambdaB,D,v):\n",
    "    l_beta = l_lambdaB + l_lambdaD\n",
    "    l_alpha = l_lambdaB/(l_beta)    \n",
    "    return(l_alpha,l_beta,D,v)\n",
    "\n",
    "def first_estimate_ple_averages_inc(l_emp_points,l_tau_list):\n",
    "    if l_tau_list[1] != 2 * l_tau_list[0]:\n",
    "        print(\"please use consecutive taus\")\n",
    "    if l_tau_list[2] != 2 * l_tau_list[1]:\n",
    "        print(\"please use consecutive taus\")\n",
    "    \n",
    "    lmbda_b = -(np.log(np.sqrt( (-l_emp_points[2] + 2*l_emp_points[1])/(-l_emp_points[1] + 2*l_emp_points[0])  ) - 1))/(l_tau_list[0])\n",
    "    lk = np.exp(-lmbda_b*l_tau_list[0])\n",
    "    \n",
    "    C11 =  (-l_emp_points[1] + 2*l_emp_points[0]) /(1-lk)**2\n",
    "    C12 =  (-l_emp_points[2] + 2*l_emp_points[1]) /(1-lk**2)**2\n",
    "    l_C1 = (C11 + C12)/2\n",
    "    \n",
    "    C21 = (l_emp_points[0] - l_C1*(1-lk))/(l_tau_list[0])\n",
    "    C22 = (l_emp_points[1] - l_C1*(1-lk**2))/(2*l_tau_list[0])\n",
    "    C23 = (l_emp_points[2] - l_C1*(1-lk**4))/(4*l_tau_list[0])\n",
    "    \n",
    "    l_C2 = (C21 + C22 + C23)/3\n",
    "    l_valid = 0\n",
    "    if lmbda_b>0:\n",
    "        if l_C2>0:\n",
    "            l_valid = 1\n",
    "    \n",
    "    return([l_C1,l_C2,lmbda_b],l_valid)\n",
    "\n",
    "def optimize(l_emp_points,l_tau_list,l_p0,max_iter,cutoff):\n",
    "    init_ratio = 0.8\n",
    "    C1_i,C2_i,lm_i = l_p0\n",
    "    l_emp_points = np.log2(l_emp_points)\n",
    "    log_tau_list = np.log2(l_tau_list)\n",
    "    tau_log_incr =np.diff(log_tau_list)\n",
    "    \n",
    "    first_min_found = False\n",
    "    \n",
    "    if np.sum(tau_log_incr!=1)>0:\n",
    "        print(\"tau list should be exponential with base 2\")\n",
    "#    print(l_emp_points)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        ratio = init_ratio+1\n",
    "        C1_i,C2_i,lm_i =  C1_i/ratio,C2_i/ratio,lm_i/ratio\n",
    "        error_list = []\n",
    "        for factor1 in range(3):\n",
    "            for factor2 in range(3):\n",
    "                for factor3 in range(3):    \n",
    "                    new_fit = log2_moment_scaling(l_tau_list,C1_i*(ratio**factor1),C2_i*(ratio**factor2),lm_i*(ratio**factor3))\n",
    "                    new_fit[np.isnan(new_fit)] = 0\n",
    "                    error_list.append(np.mean(( np.array(new_fit)-l_emp_points )**2))\n",
    "#                    if factor1==factor2==factor3==1:\n",
    "#                        print(log2_moment_scaling(l_tau_list,C1_i*(ratio**factor1),C2_i*(ratio**factor2),lm_i*(ratio**factor3)))\n",
    "#                        print(C1_i*(ratio**factor1),C2_i*(ratio**factor2),lm_i*(ratio**factor3))\n",
    "        \n",
    "#        print(error_list)\n",
    "        \n",
    "        index_opt = np.argmin(error_list)\n",
    "        opt_f1 = int(index_opt/9)\n",
    "        opt_f2 = int( (index_opt%9)/3 )\n",
    "        opt_f3 = int( index_opt%3 )\n",
    "        C1_i = C1_i*(ratio**opt_f1)\n",
    "        C2_i = C2_i*(ratio**opt_f2)\n",
    "        lm_i = lm_i*(ratio**opt_f3)\n",
    "#        print(opt_f1,opt_f2,opt_f3)\n",
    "        \n",
    "        if index_opt == 13:\n",
    "            init_ratio = init_ratio/1.9\n",
    "            first_min_found = True\n",
    "        else:\n",
    "            if not first_min_found:\n",
    "                init_ratio = 1.9*init_ratio\n",
    "                if i>cutoff:\n",
    "                    return(C1_i,C2_i,lm_i)\n",
    "                \n",
    "    return(C1_i,C2_i,lm_i)\n",
    "\n",
    "def Sergyi_expr_simp1(l_tau,l_alpha,l_beta,D,v):\n",
    "    \n",
    "    C1 = 2*((v/l_beta)**2) * (l_alpha - 1)/(l_alpha**2)\n",
    "    C2 = 2*((1-l_alpha)/l_alpha) * ((v**2)/l_beta) + 4*D*l_alpha\n",
    "    \n",
    "    return(C1*(1-np.exp(-l_alpha*l_beta*l_tau)) + C2*l_tau )\n",
    "\n",
    "def moment_scaling(tau_l,C1,C2,l_lambdaB):\n",
    "    return(C1*(np.ones(len(tau_l))-np.exp(-l_lambdaB*np.array(tau_l)))  + C2*np.array(tau_l) )\n",
    "\n",
    "def log2_moment_scaling(tau_l,C1,C2,l_lambdaB):\n",
    "    return(np.log2(C1*(np.ones(len(tau_l))-np.exp(-l_lambdaB*np.array(tau_l)))  + C2*np.array(tau_l) ))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def C2_new2(l_alpha,l_beta,D,v):\n",
    "    C2 = 2*((1-l_alpha)/l_alpha) * ((v**2)/l_beta) + 2*D*l_alpha\n",
    "    return(C2)\n",
    "    \n",
    "def C3_new2(l_alpha,l_beta,D,v):\n",
    "    C3 = 2*((1-l_alpha)/(l_alpha**2)) *((v/l_beta)**2)\n",
    "    return(C3)\n",
    "def C1_new2(l_alpha,l_beta,D,v):\n",
    "    C1 = 2*((v/l_beta)**2) * (l_alpha - 1)/(l_alpha**2)\n",
    "    return(C1)\n",
    "\n",
    "\n",
    "def intermittent2(nt,dt,mean_bal_sac,diffusion,rate12,rate21):\n",
    "\n",
    "    P1 = rate21/(rate12+rate21)\n",
    "    if np.random.random()<P1:\n",
    "        regime=1\n",
    "        waitt   = -math.log(1.0-np.random.uniform(0.0,1.0,None))/rate12\n",
    "    else:\n",
    "        regime=2\n",
    "        angle2 = np.random.randint(2)*math.pi\n",
    "        waitt   = -math.log(1.0-np.random.uniform(0.0,1.0,None))/rate21\n",
    "\n",
    "    dts = math.sqrt(dt)\n",
    "    x = np.zeros(nt)\n",
    "    y = np.zeros(nt)\n",
    "    time_since_last_jump = 0\n",
    "    for i in range(1,nt):\n",
    "        angle = random.random()*2*math.pi\n",
    "        time_since_last_jump += dt        \n",
    "        if regime == 1:\n",
    "            #diffu = diffusion*np.random.normal(0,1)*dts\n",
    "            dx = diffusion*np.random.normal(0,1)*dts\n",
    "            dy = diffusion*np.random.normal(0,1)*dts\n",
    "            x[i] = x[i-1] + dx\n",
    "            y[i] = y[i-1] + dy\n",
    "            if time_since_last_jump> waitt:\n",
    "                waitt   = -math.log(1.0-np.random.uniform(0.0,1.0,None))/rate21\n",
    "                regime = 2\n",
    "                angle2 = angle\n",
    "                time_since_last_jump =0\n",
    "        if regime == 2:\n",
    "            angle3 = angle2 \n",
    "            bal = mean_bal_sac*dt \n",
    "            dx = bal*math.cos(angle3)\n",
    "            dy = bal*math.sin(angle3)\n",
    "            x[i] = x[i-1] + dx\n",
    "            y[i] = y[i-1] + dy\n",
    "\n",
    "            if time_since_last_jump> waitt:\n",
    "                waitt   = -math.log(1.0-np.random.uniform(0.0,1.0,None))/rate12\n",
    "                time_since_last_jump =0\n",
    "                regime = 1\n",
    "        \n",
    "    return(x,y)\n",
    "\n",
    "def levy_flight_2D_2(n_redirections,n_max,lalpha,tmin,measuring_dt):\n",
    "    if lalpha <= 1:\n",
    "        print(\"alpha should be larger than 1\")\n",
    "        return(\"alpha should be larger than 1\")\n",
    "    \n",
    "\n",
    "    t_redirection= tmin*(np.ones(n_redirections) - np.random.rand(n_redirections))**(1.0/(-lalpha+1))   \n",
    "    cum_t_redirection = np.cumsum(t_redirection)\n",
    "\n",
    "\n",
    "    angle = np.random.rand(len(t_redirection))*2*math.pi\n",
    "    x_increments = t_redirection*np.cos(angle)\n",
    "    y_increments = t_redirection*np.sin(angle)\n",
    "    l_x_list = np.cumsum(x_increments)\n",
    "    l_y_list = np.cumsum(y_increments)\n",
    "    \n",
    "\n",
    "    if n_max*measuring_dt < cum_t_redirection[-1]:\n",
    "\n",
    "        x_measured = np.interp(np.arange(0,n_max*measuring_dt,measuring_dt),np.cumsum(t_redirection),l_x_list)\n",
    "        y_measured = np.interp(np.arange(0,n_max*measuring_dt,measuring_dt),np.cumsum(t_redirection),l_y_list)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        n_max = int(cum_t_redirection[-1]/measuring_dt)\n",
    "        #print(\"me<zasuring time greater than simulated time. n_max becomes \" + str(n_max))\n",
    "        x_measured = np.interp(np.arange(0,n_max*measuring_dt,measuring_dt),np.cumsum(t_redirection),l_x_list)\n",
    "        y_measured = np.interp(np.arange(0,n_max*measuring_dt,measuring_dt),np.cumsum(t_redirection),l_y_list)\n",
    "        #print(\"measuring time greater than simulated time.\")\n",
    "    return x_measured,y_measured,t_redirection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ba3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_two_vector(ldx,ldy):\n",
    "    if np.any(np.isnan(ldx)):\n",
    "        print(\"nan in input vector\")\n",
    "        return(\"nan in input vector\")\n",
    "    if np.any(np.isnan(ldy)):\n",
    "        print(\"nan in input vector\")\n",
    "        return(\"nan in input vector\")\n",
    "    if len(ldx) != len(ldy):\n",
    "        print(\"len of vectors should be the same\")\n",
    "        return( \"len of vectors should be the same\")\n",
    "    angle_vec = []\n",
    "    ldx_1 = np.array(ldx[1:])\n",
    "    ldx_2 = np.array(ldx[:-1])\n",
    "    ldy_1 = np.array(ldy[1:])\n",
    "    ldy_2 = np.array(ldy[:-1])\n",
    "    angle_vec1 = np.angle(ldx_1 + ldy_1*1.0j,deg = True)\n",
    "    angle_vec2 = np.angle(ldx_2 + ldy_2*1.0j,deg = True)\n",
    "\n",
    "    angle_diff = angle_vec2-angle_vec1\n",
    "\n",
    "    angle_diff = angle_diff + 360*np.ones(len(angle_diff))*np.array(angle_diff<0) - 360*np.ones(len(angle_diff))*np.array(angle_diff>360) \n",
    " \n",
    "\n",
    "    return(angle_diff)\n",
    "\n",
    "def angle_single_vector(ldx,ldy):\n",
    "    if np.any(np.isnan(ldx)):\n",
    "        print(  \"nan in input vector\")\n",
    "        return( \"nan in input vector\")\n",
    "    if np.any(np.isnan(ldy)):\n",
    "        print( \"nan in input vector\")\n",
    "        return( \"nan in input vector\")\n",
    "    if len(ldx) != len(ldy):\n",
    "        print( \"len of vectors should be the same\")\n",
    "        return( \"len of vectors should be the same\")\n",
    "    angle_vec = []\n",
    "    ldx = np.array(ldx)\n",
    "    ldy = np.array(ldy)\n",
    "    angle_vec = np.angle(ldx + ldy*1.0j,deg = True)\n",
    "\n",
    "    return( angle_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = '/Users/pedroreg/Desktop/research copy/Flicker frequency'\n",
    "\n",
    "#data_F0 = [] #no flickering detected\n",
    "#data_F1 = [] #60 hz flickering detected\n",
    "#data_F2 = [] #120 hz flickering detected\n",
    "\n",
    "group00 = []\n",
    "group10 = []\n",
    "group11 = []\n",
    "all_data = []\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if 'csv' in filename:\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                if 'T120_1' in file_path:\n",
    "                    all_data.append(pd.read_csv(file_path))\n",
    "                    group00.append(0)\n",
    "                    group10.append(1)\n",
    "                    group11.append(1)\n",
    "                elif 'T60_1' in file_path:\n",
    "                    all_data.append(pd.read_csv(file_path))\n",
    "                    group00.append(0)\n",
    "                    group10.append(1)\n",
    "                    group11.append(0)\n",
    "                elif 'T60_0'in file_path:\n",
    "                    all_data.append(pd.read_csv(file_path))\n",
    "                    group00.append(1)\n",
    "                    group10.append(0)\n",
    "                    group11.append(0)\n",
    "                else:\n",
    "                    print('ERROR, NO F')\n",
    "                #print(f'Data in {filename}:\\n{data}\\n')\n",
    "            except Exception as e:\n",
    "                print(f'Error reading {filename}: {e}')\n",
    "group00 = np.array(group00).astype('bool')\n",
    "group10 = np.array(group10).astype('bool')\n",
    "group11 = np.array(group11).astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577310f-47b9-434a-a38c-363a9997122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = []\n",
    "X = []\n",
    "Y = []\n",
    "d_X = []\n",
    "d_Y = []\n",
    "XX = []\n",
    "YY = []\n",
    "TT = []\n",
    "d_T = []\n",
    "d_S = []\n",
    "dT = []\n",
    "dX = []\n",
    "dY = []\n",
    "dS = []\n",
    "saccade = []\n",
    "Ssaccade = []\n",
    "vS = []\n",
    "remove_nanbefore = 200\n",
    "\n",
    "            \n",
    "\n",
    "for i in range(0,len(all_data)):\n",
    "    cont = 0\n",
    "\n",
    "\n",
    "    T.append(np.swapaxes(np.array(all_data[i]),0,1)[0])\n",
    "    X.append(np.swapaxes(np.array(all_data[i]),0,1)[1])\n",
    "    Y.append(np.swapaxes(np.array(all_data[i]),0,1)[2])\n",
    "    saccade.append(np.swapaxes(np.array(all_data[i]),0,1)[3])\n",
    "    \n",
    "    d_X.append(np.diff(X[-1]))\n",
    "    d_Y.append(np.diff(Y[-1]))\n",
    "    d_T.append(np.diff(T[-1]))\n",
    "    d_S.append(np.sqrt(np.diff(X[-1])**2 + np.diff(Y[-1])**2))\n",
    "\n",
    "    #remove NaN\n",
    "    Nan_array = np.array(np.logical_not(np.isnan(d_X[-1])))*np.array(np.logical_not(np.isnan(d_Y[-1])))\n",
    "    new_nan_array = copy.copy(Nan_array)\n",
    "    for i in range(1,remove_nanbefore):\n",
    "        new_nan_array = new_nan_array[:-1]*Nan_array[i:]\n",
    "    Nan_array = new_nan_array\n",
    "    \n",
    "    for i in range(1,remove_nanbefore):\n",
    "        new_nan_array = new_nan_array[1:]*Nan_array[:-i]\n",
    "    Nan_array = new_nan_array\n",
    "    Nan_array = np.append(Nan_array,np.zeros(remove_nanbefore-1))\n",
    "    Nan_array = np.append(np.zeros(remove_nanbefore-1),Nan_array)\n",
    "    Nan_array = Nan_array.astype(bool)\n",
    "\n",
    "\n",
    "    dX.append(d_X[-1][Nan_array])\n",
    "    dY.append(d_Y[-1][Nan_array])\n",
    "    dS.append(np.sqrt(np.array(dX[-1])**2 + np.array(dY[-1])**2))\n",
    "    dT.append(d_T[-1][Nan_array])\n",
    "    vS.append(np.array(dS[-1])/np.array(dT[-1]))\n",
    "    XX.append(np.array(X[-1][:-1])[Nan_array])\n",
    "    YY.append(np.array(Y[-1][:-1])[Nan_array])\n",
    "    Ssaccade.append(np.array(saccade[-1][:-1])[Nan_array])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cbb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_spectrum(vector,frequency,graph,label):\n",
    "    local_FT = np.fft.rfft(vector)\n",
    "    local_P_S = np.square(np.abs(local_FT))\n",
    "    frequencies = np.linspace(0, frequency/2, len(local_P_S))\n",
    "    if graph:\n",
    "        plt.plot(frequencies,local_P_S)\n",
    "        #plt.plot(local_P_S)\n",
    "        plt.title(\"Power Spectrum of \" + label)\n",
    "        plt.xlabel(\"frequency (HZ)\")\n",
    "        #plt.savefig(\"power_spectrum\"+label+\".png\",dpi=500)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return( local_P_S )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da947aba-c77c-4f8c-a620-3af08b0812ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4745cc4b-981a-456b-9d2b-0d6a87dc6d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b992b1d-ff20-42b3-b345-68031c64cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Greater = np.array([1,np.sum(group00),np.sum(group11==0)] )/81\n",
    "Lesser = np.array([80,np.sum(group10),np.sum(group11)] )/81\n",
    "  \n",
    "spacing2 = 0.3\n",
    "r = np.arange(0,3*spacing2,spacing2) \n",
    "width = 0.2\n",
    "spacing = 0.1\n",
    "\n",
    "#014E20\n",
    "#02B07B\n",
    "\n",
    "color_palette = ['#014E20','#02B07B','#01204E','#028391','#811638','#FAA968']\n",
    "  \n",
    "#plt.bar(r, Greater, color = [color_palette[0],color_palette[2],color_palette[4]], \n",
    "#        width = width, edgecolor = 'black') \n",
    "plt.bar(r + spacing + width, Lesser, color = [color_palette[1],color_palette[3],color_palette[5]], \n",
    "        width = width, edgecolor = 'black') \n",
    "\n",
    "plt.xticks([width+spacing,spacing2+width+spacing,2*spacing2+0.5*width + 2*spacing] ,[r'$\\mathcal{P}_{>30}$',r'$\\mathcal{P}_{>60}$',r'$\\mathcal{P}_{>120}$']) \n",
    "  \n",
    "plt.ylabel('Fraction of Participants')\n",
    "plt.savefig('Nr_participants5.png',dpi=300)\n",
    "  \n",
    "# plt.grid(linestyle='--') \n",
    "#plt.xticks(r + width/2,['2018','2019','2020','2021']) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0f039-046c-47f1-a581-5a67a321c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_two_vector_tot = []\n",
    "Phi_two_vector_less_60 = []\n",
    "Phi_two_vector_greater_60 = []\n",
    "Phi_two_vector_less_120 = []\n",
    "Phi_two_vector_greater_120 = []\n",
    "for i in range(1,50):\n",
    "    #print(i)\n",
    "    Phi_two_vector_tot.append([])\n",
    "    Phi_two_vector_less_60.append([])\n",
    "    Phi_two_vector_greater_60.append([])\n",
    "    Phi_two_vector_less_120.append([])\n",
    "    Phi_two_vector_greater_120.append([])\n",
    "    for k in range(len(dX)):\n",
    "        Phi_two_vector_tot[-1].append(angle_two_vector(dX[k][::i],dY[k][::i]))\n",
    "        if k in np.where(group00)[0]:\n",
    "            Phi_two_vector_less_60[-1].append(angle_two_vector(dX[k][::i],dY[k][::i]))\n",
    "        if k in np.where(group10)[0]:\n",
    "            Phi_two_vector_greater_60[-1].append(angle_two_vector(dX[k][::i],dY[k][::i]))\n",
    "        if k in np.where(group11==0)[0]:\n",
    "            Phi_two_vector_less_120[-1].append(angle_two_vector(dX[k][::i],dY[k][::i]))\n",
    "        if k in np.where(group11)[0]:\n",
    "            Phi_two_vector_greater_120[-1].append(angle_two_vector(dX[k][::i],dY[k][::i]))\n",
    "\n",
    "    Phi_two_vector_tot[-1] = np.hstack(Phi_two_vector_tot[-1])\n",
    "    Phi_two_vector_less_60[-1] = np.hstack(Phi_two_vector_less_60[-1])\n",
    "    Phi_two_vector_greater_60[-1] = np.hstack(Phi_two_vector_greater_60[-1])\n",
    "    Phi_two_vector_less_120[-1] = np.hstack(Phi_two_vector_less_120[-1])\n",
    "    Phi_two_vector_greater_120[-1] = np.hstack(Phi_two_vector_greater_120[-1])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5088e9-1d0f-490e-865f-b7c04f452b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lranges = [360/2,360/4,360/8]\n",
    "lcolors = ['#387FC8', '#F3B552','#393943' ]\n",
    "labels = [r'$k=2$', '$k=4$','$k=8$' ]\n",
    "lenarr = 10\n",
    "\n",
    "PPtot_list = []\n",
    "\n",
    "for l in range(len(lranges)):\n",
    "\n",
    "    PPtot = []\n",
    "    #PPge120 = []\n",
    "    #PPle120 = []\n",
    "    #PPge60 = []\n",
    "    #PPle60 = []\n",
    "    #lrange = 180\n",
    "    for i in np.arange(lenarr):\n",
    "        PPtot.append(np.sum( (Phi_two_vector_tot[i]>180-0.5*lranges[l])*(Phi_two_vector_tot[i]<180+0.5*lranges[l]) )/len(Phi_two_vector_tot[i]))\n",
    "        #PPge120.append(np.sum( (Phi_two_vector_greater_120[i]>180-0.5*lrange)*(Phi_two_vector_greater_120[i]<180+0.5*lrange) )/len(Phi_two_vector_greater_120[i]))\n",
    "        #PPle120.append(np.sum( (Phi_two_vector_less_120[i]>180-0.5*lrange)*(Phi_two_vector_less_120[i]<180+0.5*lrange) )/len(Phi_two_vector_less_120[i]))\n",
    "        #PPge60.append(np.sum( (Phi_two_vector_greater_60[i]>180-0.5*lrange)*(Phi_two_vector_greater_60[i]<180+0.5*lrange) )/len(Phi_two_vector_greater_60[i]))\n",
    "        #PPle60.append(np.sum( (Phi_two_vector_less_60[i]>180-0.5*lrange)*(Phi_two_vector_less_60[i]<180+0.5*lrange) )/len(Phi_two_vector_less_60[i]))\n",
    "    PPtot_list.append(PPtot)\n",
    "    plt.plot(np.arange(1,lenarr+1),PPtot,c=lcolors[l],label = labels[l])\n",
    "    plt.plot(np.arange(1,lenarr+1),PPtot,'.',c=lcolors[l])\n",
    "    #plt.plot(np.arange(1,31),PPge120,c='#FAA968', label=r'$\\mathcal{P}_{>120}$')\n",
    "    plt.xlabel(r'$\\tau$ (ms)')\n",
    "    #plt.ylabel(r'$Pr(\\phi \\in [0.9\\pi,1.1\\pi]$')\n",
    "    plt.ylabel(r'$P_{\\phi,k}$',fontsize=20)\n",
    "    plt.legend(loc='right')\n",
    "    #plt.text(0, 0.4, r'$t^{\\prime} = 2.5$ms', fontsize=15)\n",
    "    #plt.text(3.55, 0.35, r'$t^{\\prime} = 3.5$ms', fontsize=15)\n",
    "    plt.text(3.05, 0.45, r'$\\tau = 3$ms', fontsize=15)\n",
    "    plt.vlines(2.5,-0,0.59,linestyle='dotted',color='dimgrey',alpha=0.5)\n",
    "    plt.vlines(3.5,-0,0.59,linestyle='dotted',color='dimgrey',alpha=0.5)\n",
    "    plt.vlines(3,-0,0.59,linestyle='--',color='dimgrey',alpha=0.5)\n",
    "    plt.ylim([0,0.59])\n",
    "    #plt.hlines(lrange/360,1,30,linestyle='--',color='dimgrey',alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig('pr_phi_combo.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6b046-8143-4d27-a33e-f992031021f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "textt = [\"a)\",\"b)\",\"c)\"]\n",
    "for j in np.arange(3):\n",
    "    i = [1,3,7][j]\n",
    "    kde = stats.gaussian_kde(1.1*np.abs(np.abs(2*math.pi*(Phi_two_vector_tot[i])/360 -math.pi)-math.pi)-0.12,bw_method=0.1)\n",
    "    bins = np.arange(0.05,math.pi-0.05,math.pi/100)\n",
    "    plt.plot(bins,kde(bins),c='k')\n",
    "    #plt.hist(np.abs(np.abs(2*math.pi*(Phi_two_vector_tot[i])/360 -math.pi)-math.pi),np.arange(-0.05*math.pi,1.05*math.pi,math.pi/20),density = True);\n",
    "    print(i)\n",
    "    plt.xlabel(r'$\\phi$')\n",
    "    plt.ylabel(r'Density')\n",
    "    plt.title(r'$t^{\\prime} = $'+str(i+1) + ' ms')\n",
    "    plt.xticks(ticks=([0,math.pi/4,math.pi/2,0.75*math.pi,math.pi]) ,labels=(['0',r'$\\pi/4$',r'$\\pi/2$',r'$3\\pi/4$',r'$\\pi$']))\n",
    "    plt.tight_layout()\n",
    "    plt.text(2.8, max(kde(bins)), textt[j], ha='right', va='top', fontsize=20)\n",
    "    plt.savefig('phi_density_t='+str(i+1)+'.png',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868efd89-95dc-4350-ba4b-b573e76f3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table = np.zeros([6,16]).astype(list) #varname, 60 frequency , 60 p-val, 120 frequency , 120 p-val,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfd7e1-13c2-4291-9578-edc2b8f293ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Using normalized velocity ########################\n",
    "\n",
    "cum_list = []\n",
    "Energy = []\n",
    "N = 1\n",
    "skip_lag = 1\n",
    "frequency = 1000/skip_lag\n",
    "i_f_normed_dx = []\n",
    "f_normed_dx = []\n",
    "\n",
    "for k in range(len(dX)):\n",
    "    norm_dx = np.cos(np.array(angle_single_vector(np.diff(XX[k][::skip_lag]),np.diff(YY[k][::skip_lag])))*math.pi/180)\n",
    "    l_P_S = power_spectrum(norm_dx,frequency,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    cummulative = np.cumsum(moving_averages)/np.sum(moving_averages)\n",
    "    plt.plot(frequencies,cummulative)\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "    #print(cummulative[int( len(moving_averages)*60*skip_lag/frequency)])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.2)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.4)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.6)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.8)[0][0]])\n",
    "    i_f_normed_dx.append([])\n",
    "    f_normed_dx.append([])\n",
    "    for fff in np.arange(0,1,0.001):\n",
    "        i_f_normed_dx[-1].append(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>=fff)[0][0]])\n",
    "        f_normed_dx[-1].append(cummulative[int(len(cummulative)*fff)])\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))\n",
    "f_normed_dx = np.array(f_normed_dx)\n",
    "i_f_normed_dx = np.array(i_f_normed_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59987563-877a-4047-9ac7-ac051e564a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_i_f_normed_dx = np.swapaxes(i_f_normed_dx,0,1)\n",
    "s_f_normed_dx = np.swapaxes(f_normed_dx,0,1)\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_i_f_normed_dx)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_i_f_normed_dx[i][group00],s_i_f_normed_dx[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_f_normed_dx[i][group00],s_f_normed_dx[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(f_normed_dx[group00],axis=0)-np.mean(f_normed_dx[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_i_f_normed_dx[index_fopt60]\n",
    "selected_f_normed_dx60 = s_f_normed_dx[index_fopt60]\n",
    "print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][0] = 'Ps $v_{x,unit}$'\n",
    "table[1][0] = index_fopt60*500*0.001\n",
    "table[2][0] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "\n",
    "s_i_f_normed_dx = np.swapaxes(i_f_normed_dx,0,1)\n",
    "s_f_normed_dx = np.swapaxes(f_normed_dx,0,1)\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_i_f_normed_dx)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_i_f_normed_dx[i][group00],s_i_f_normed_dx[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_f_normed_dx[i][group00],s_f_normed_dx[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(f_normed_dx[group00],axis=0)-np.mean(f_normed_dx[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_i_f_normed_dx[index_fopt120]\n",
    "selected_f_normed_dx120 = s_f_normed_dx[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][0] = index_fopt120*500*0.001\n",
    "table[4][0] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53c1f841-c217-440a-a9be-8dae8f9ee8c7",
   "metadata": {},
   "source": [
    "color_palette = ['#01204E','#028391','#811638','#FAA968']\n",
    "index = index_fopt60\n",
    "\n",
    "vec = s_f_normed_dx\n",
    "\n",
    "plt.errorbar(np.linspace(0,500,1000),np.mean(np.swapaxes(vec,0,1)[group00],axis=0),yerr=2*np.std(np.swapaxes(vec,0,1)[group00],axis=0)/np.sqrt(np.sum(group00)),c = color_palette[2], label=r'$\\mathcal{P}_{<120}$')\n",
    "plt.errorbar(np.linspace(0,500,1000),np.mean(np.swapaxes(vec,0,1)[group11],axis=0),yerr=2*np.std(np.swapaxes(vec,0,1)[group11],axis=0)/np.sqrt(np.sum(group11)),c = color_palette[3], label=r'$\\mathcal{P}_{>120}$',alpha=0.2)\n",
    "\n",
    "plt.xlabel(r'$f^{\\prime}$ (Hz)',fontsize = 25)\n",
    "plt.ylabel(r'$\\langle C_{v_{x,\\text{unit}}}(f^{\\prime})\\rangle_{m}$',fontsize = 25)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('PS_C_group60.pdf',dpi=300,format=\"pdf\", bbox_inches=\"tight\" )\n",
    "plt.show()\n",
    "\n",
    "index = index_fopt120\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(0,500,1000),np.median(np.swapaxes(s_f_normed_dx,0,1)[group00],axis=0),c = color_palette[2], label=r'$\\mathcal{P}_{<60}$')\n",
    "plt.plot(np.linspace(0,500,1000),np.median(np.swapaxes(s_f_normed_dx,0,1)[group00==0],axis=0),c = color_palette[3], label=r'$\\mathcal{P}_{>60}$')\n",
    "plt.xlabel(r'$f^{\\prime}$ (Hz)',fontsize = 25)\n",
    "plt.ylabel(r'$\\langle C_{v_{x,\\text{unit}}}(f^{\\prime})\\rangle_{m}$',fontsize = 25)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('PS_C_group120.pdf',dpi=300,format=\"pdf\", bbox_inches=\"tight\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860b5df-ca08-4dbe-842f-3b9d87ee147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Using normalized velocity ########################\n",
    "\n",
    "\n",
    "Energy = []\n",
    "N = 1\n",
    "skip_lag = 1\n",
    "frequency = 1000/skip_lag\n",
    "i_f_normed_dy = []\n",
    "f_normed_dy = []\n",
    "\n",
    "for k in range(len(dX)):\n",
    "    norm_dy = np.sin(np.array(angle_single_vector(np.diff(XX[k][::skip_lag]),np.diff(YY[k][::skip_lag])))*math.pi/180)\n",
    "    l_P_S = power_spectrum(norm_dy,frequency,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    cummulative = np.cumsum(moving_averages)/np.sum(moving_averages)\n",
    "    plt.plot(frequencies,cummulative)\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "    #print(cummulative[int( len(moving_averages)*60*skip_lag/frequency)])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.2)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.4)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.6)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.8)[0][0]])\n",
    "    i_f_normed_dy.append([])\n",
    "    f_normed_dy.append([])\n",
    "    for fff in np.arange(0,1,0.001):\n",
    "        i_f_normed_dy[-1].append(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>=fff)[0][0]])\n",
    "        f_normed_dy[-1].append(cummulative[int(len(cummulative)*fff)])\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fe8bf-ed4a-4aae-8737-5580ff25b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = f_normed_dy\n",
    "ivec = i_f_normed_dy\n",
    "table_index = 1\n",
    "varname = 'Ps $v_{y,unit}$'\n",
    "\n",
    "vec = np.array(vec)\n",
    "ivec = np.array(ivec)\n",
    "s_ivec = np.swapaxes(ivec,0,1)\n",
    "s_vec = np.swapaxes(vec,0,1)\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][table_index] = varname\n",
    "table[1][table_index] = index_fopt60*500*0.001\n",
    "table[2][table_index] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_i_f_normed_dx)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][table_index] = index_fopt120*500*0.001\n",
    "table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd5e1d-10f4-4fd5-b7d1-5bc53ce8cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = ['#01204E','#028391','#811638','#FAA968']\n",
    "index = index_fopt60\n",
    "\n",
    "plt.errorbar(np.linspace(0,500,1000),np.mean(vec[group00],axis=0),yerr=np.std(vec[group00],axis=0)/np.sqrt(np.sum(group00)),c = color_palette[0], label=r'$\\mathcal{P}_{<60}$')\n",
    "plt.errorbar(np.linspace(0,500,1000),np.mean(vec[group10],axis=0),yerr=np.std(vec[group10],axis=0)/np.sqrt(np.sum(group10)),c = color_palette[1], label=r'$\\mathcal{P}_{>60}$',alpha=1)\n",
    "plt.xlabel(r'$f^{\\prime}$ (Hz)',fontsize = 25)\n",
    "plt.ylabel(r'$\\langle C_{v_{y,\\text{unit}}}(f^{\\prime})\\rangle$',fontsize = 25)\n",
    "plt.vlines(index*500*0.001,0,1,linestyle='--',color='dimgrey')\n",
    "plt.annotate(r'$f^*$',(index*500*0.001+5,0.05),fontsize = 25)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('PS_C_group60.pdf',dpi=300,format=\"pdf\", bbox_inches=\"tight\" )\n",
    "plt.show()\n",
    "\n",
    "index = index_fopt120\n",
    "\n",
    "bins = np.arange(0,0.3,0.001)\n",
    "plt.errorbar(np.linspace(0,500,1000),np.mean(vec[group00],axis=0),yerr=np.std(vec[group00],axis=0)/np.sqrt(np.sum(group00)),c = color_palette[2], label=r'$\\mathcal{P}_{<120}$')\n",
    "plt.errorbar(np.linspace(0,500,1000),np.mean(vec[group11],axis=0),yerr=np.std(vec[group11],axis=0)/np.sqrt(np.sum(group11)),c = color_palette[3], label=r'$\\mathcal{P}_{>120}$',alpha=1)\n",
    "plt.vlines(index*500*0.001,0,1,linestyle='--',color='dimgrey')\n",
    "plt.annotate(r'$f^*$',(index*500*0.001+5,0.05),fontsize = 25)\n",
    "plt.xlabel(r'$f^{\\prime}$ (Hz)',fontsize = 25)\n",
    "plt.ylabel(r'$\\langle C_{v_{y,\\text{unit}}}(f^{\\prime})\\rangle$',fontsize = 25)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('PS_C_group120.pdf',dpi=300,format=\"pdf\", bbox_inches=\"tight\" )\n",
    "plt.show()\n",
    "\n",
    "index = index_fopt60\n",
    "color_palette = ['#01204E','#028391','#811638','#FAA968']\n",
    "index = np.argmin(pvals_f)\n",
    "\n",
    "kde00 = stats.gaussian_kde(s_vec[index][group00],bw_method=0.2)\n",
    "kde10 = stats.gaussian_kde(s_vec[index][group10],bw_method=0.2)\n",
    "#kde11 = stats.gaussian_kde(s_f_normed_dy[index][group11==1],bw_method=0.2)\n",
    "bins = np.arange(0,0.4,0.001)\n",
    "plt.plot(bins,kde00(bins),c = color_palette[0], label=r'$\\mathcal{P}_{<60}$')\n",
    "plt.plot(bins,kde10(bins),c = color_palette[1], label=r'$\\mathcal{P}_{>60}$')\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel(r'$C_{v_{y,\\text{unit}}}(f^*=' + str(index_fopt60*500*0.001) +r'\\text{Hz})$',fontsize = 21)\n",
    "#plt.plot(bins,kde11(bins),c = color_palette[2], label=r'$\\mathcal{P}_{>120}$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ps_group60.png',dpi=300)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "index = index_fopt120\n",
    "color_palette = ['#01204E','#028391','#811638','#FAA968']\n",
    "index = np.argmin(pvals_f)\n",
    "\n",
    "kde11 = stats.gaussian_kde(s_vec[index][group11],bw_method=0.2)\n",
    "kdeN11 = stats.gaussian_kde(s_vec[index][group00],bw_method=0.2)\n",
    "\n",
    "bins = np.arange(0,0.4,0.001)\n",
    "plt.plot(bins,kdeN11(bins),c = color_palette[2], label=r'$\\mathcal{P}_{<120}$')\n",
    "plt.plot(bins,kde11(bins),c = color_palette[3], label=r'$\\mathcal{P}_{>120}$')\n",
    "#plt.plot(bins,kde11(bins),c = color_palette[2], label=r'$\\mathcal{P}_{>120}$')\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel(r'$C_{v_{y,\\text{unit}}}(f^*=' + str(index_fopt60*500*0.001) +r'\\text{Hz})$',fontsize = 21)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ps_group120.png',dpi=300)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1f72d-623c-45d5-8f8a-7757617b5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vec[index][group11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c872f3-532f-4694-9241-838bf048b34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b290c67-3299-4656-82c7-b4cba775ed92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################## Using fixation velocity ########################\n",
    "\n",
    "\n",
    "Energy = []\n",
    "N = 1\n",
    "skip_lag = 1\n",
    "frequency = 1000/skip_lag\n",
    "i_f_fix_dx = []\n",
    "f_fix_dx = []\n",
    "count = 0\n",
    "for k in range(len(dX)):\n",
    "    if np.sum(Ssaccade[k])>0:\n",
    "        norm_dx = dX[k][Ssaccade[k]==1]\n",
    "    else:\n",
    "        norm_dx = dX[k]\n",
    "        count +=1\n",
    "    l_P_S = power_spectrum(norm_dx,frequency,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    cummulative = np.cumsum(moving_averages)/np.sum(moving_averages)\n",
    "    plt.plot(frequencies,cummulative)\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "    #print(cummulative[int( len(moving_averages)*60*skip_lag/frequency)])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.2)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.4)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.6)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.8)[0][0]])\n",
    "    i_f_fix_dx.append([])\n",
    "    f_fix_dx.append([])\n",
    "    for fff in np.arange(0,1,0.001):\n",
    "        i_f_fix_dx[-1].append(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>=fff)[0][0]])\n",
    "        f_fix_dx[-1].append(cummulative[int(len(cummulative)*fff)])\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c0806-6454-4b04-8e3c-06a128fc004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = f_fix_dx\n",
    "ivec = i_f_fix_dx\n",
    "table_index = 2\n",
    "varname = 'Ps $v_{x,fix}$'\n",
    "\n",
    "vec = np.array(vec)\n",
    "ivec = np.array(ivec)\n",
    "s_ivec = np.swapaxes(ivec,0,1)\n",
    "s_vec = np.swapaxes(vec,0,1)\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "selected_f_normed_dx60 = vec[index_fopt60]\n",
    "print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][table_index] = varname\n",
    "table[1][table_index] = index_fopt60*500*0.001\n",
    "table[2][table_index] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_i_f_normed_dx)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][table_index] = index_fopt120*500*0.001\n",
    "table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54459b68-3fc0-4caf-9563-50a84f99e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "['#01204E','#028391','#FAA968']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac76e1-a202-42f4-aecb-aa0b2098abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Using fixation velocity ########################\n",
    "\n",
    "Energy = []\n",
    "N = 1\n",
    "skip_lag = 1\n",
    "frequency = 1000/skip_lag\n",
    "i_f_fix_dy = []\n",
    "f_fix_dy = []\n",
    "\n",
    "for k in range(len(dX)):\n",
    "    norm_dx = dY[k][Ssaccade[k]==1]\n",
    "    if np.sum(Ssaccade[k])>0:\n",
    "        norm_dx = dY[k][Ssaccade[k]==1]\n",
    "    else:\n",
    "        norm_dx = dY[k]\n",
    "        count +=1\n",
    "    l_P_S = power_spectrum(norm_dx,frequency,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    cummulative = np.cumsum(moving_averages)/np.sum(moving_averages)\n",
    "    plt.plot(frequencies,cummulative)\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "    #print(cummulative[int( len(moving_averages)*60*skip_lag/frequency)])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.2)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.4)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.6)[0][0]])\n",
    "    print(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.8)[0][0]])\n",
    "    i_f_fix_dy.append([])\n",
    "    f_fix_dy.append([])\n",
    "    for fff in np.arange(0.001,1,0.001):\n",
    "        i_f_fix_dy[-1].append(frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>=fff)[0][0]])\n",
    "        f_fix_dy[-1].append(cummulative[int(len(cummulative)*fff)])\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5cb03-7a71-49c1-83ed-7e519cdd7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = f_fix_dy\n",
    "ivec = i_f_fix_dy\n",
    "table_index = 3\n",
    "varname = 'Ps $v_{y,fix}$'\n",
    "\n",
    "vec = np.array(vec)\n",
    "ivec = np.array(ivec)\n",
    "s_ivec = np.swapaxes(ivec,0,1)\n",
    "s_vec = np.swapaxes(vec,0,1)\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][table_index] = varname\n",
    "table[1][table_index] = index_fopt60*500*0.001\n",
    "table[2][table_index] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][table_index] = index_fopt120*500*0.001\n",
    "table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c455acb-3625-48e1-80fb-ab1c91ae8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887ce31-3942-4059-bc5c-fa33a772c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "imf = emd.sift.mask_sift(dY[1][Ssaccade[1]==1], max_imfs=4)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "emd.plotting.plot_imfs(imf[:1000*3, :])\n",
    "plt.savefig('imf_example.png',dpi=300)\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd508921-9654-4453-b7dd-8c219c78509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "\n",
    "f, spectrum = emd.spectra.hilberthuang(IF, IA, (0, 300, 25))\n",
    "\n",
    "freq_edges, freq_centres = emd.spectra.define_hist_bins(0, 300, 128, 'linear')\n",
    "\n",
    "# Amplitude weighted HHT per IMF\n",
    "f, spec_weighted = emd.spectra.hilberthuang(IF, IA, freq_edges, sum_imfs=False)\n",
    "\n",
    "# Unweighted HHT per IMF - we replace the instantaneous amplitude values with ones\n",
    "f, spec_unweighted = emd.spectra.hilberthuang(IF, np.ones_like(IA), freq_edges, sum_imfs=False)\n",
    "lcolors = ['#1B9E77','#7570B3','#66A61E','#A6761D']\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    norm_fac = np.sum((freq_centres[1]-freq_centres[0])*np.swapaxes(spec_unweighted,0,1)[i])\n",
    "    plt.plot(freq_centres, np.array(np.swapaxes(spec_unweighted,0,1)[i])/norm_fac,c=lcolors[i])\n",
    "plt.xticks(np.arange(5)*50)\n",
    "plt.xlim(0, 300)\n",
    "plt.xlabel(r'$\\tilde{f}$ (Hz)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Instantaneous Frequency (IF)')\n",
    "plt.legend([r'$\\rho_{IMF1,v_{x,fix}}$', r'$\\rho_{IMF2,v_{x,fix}}$', r'$\\rho_{IMF3,v_{x,fix}}$', r'$\\rho_{IMF4,v_{x,fix}}$'], frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('HH-transform.png',dpi=300)\n",
    "\n",
    "'''plt.subplot(122)\n",
    "plt.plot(freq_centres, spec_weighted)\n",
    "plt.xticks(np.arange(5)*50)\n",
    "plt.xlim(0, 300)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power')\n",
    "plt.title('IA-weighted\\nHilbert-Huang Transform')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40317220-dac2-414c-b185-473bf870aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.swapaxes(spec_unweighted,0,1)[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67b5b8-8bb1-43a4-8e0d-60123711a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### IMF 1 - fix vel #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b27a5-5240-464c-a3f7-0983d881e2ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "i_f_emd_IF_dx = []\n",
    "f_emd_IF_dx = []\n",
    "i_f_emd_IA_dx = []\n",
    "f_emd_IA_dx = []\n",
    "\n",
    "for i in range(len(dX)):\n",
    "    if np.sum(Ssaccade[i])>0:\n",
    "        imf = emd.sift.mask_sift(dX[i][Ssaccade[i]==1], max_imfs=4)\n",
    "    else:\n",
    "        imf = emd.sift.mask_sift(dX[i], max_imfs=4)\n",
    "    IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "    \n",
    "    #emd.plotting.plot_imfs(imf[:1000*3, :])\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    # Plot a simple histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 150),color=lcolor);\n",
    "    #peak_IF.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 0],bw_method=0.07)\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram')\n",
    "    plt.xticks(np.arange(0, 500, 100))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "\n",
    "    cummulative_IF = np.cumsum(ghist)/np.sum(ghist)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    # Plot an amplitude-weighted histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 100), weights=IA[:, 2],color=lcolor)\n",
    "    #peak_IA.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 0],bw_method=0.07, weights=np.abs(IA[:, 2]))\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram\\nweighted by IA')\n",
    "    plt.xticks(np.arange(0, 500, 100))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.show()\n",
    "    \n",
    "    cummulative_IA = np.cumsum(ghist)/np.sum(ghist)\n",
    "\n",
    "\n",
    "    i_f_emd_IF_dx.append([])\n",
    "    f_emd_IF_dx.append([])\n",
    "    i_f_emd_IA_dx.append([])\n",
    "    f_emd_IA_dx.append([])\n",
    "    for fff in np.arange(0,1,0.001):\n",
    "        i_f_emd_IF_dx[-1].append(gbins[np.where(cummulative_IF>=fff)[0][0]])\n",
    "        f_emd_IF_dx[-1].append(cummulative_IF[int(len(cummulative_IF)*fff)])\n",
    "        i_f_emd_IA_dx[-1].append(gbins[np.where(cummulative_IA>=fff)[0][0]])\n",
    "        f_emd_IA_dx[-1].append(cummulative_IA[int(len(cummulative_IA)*fff)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b295e-6d46-475b-8030-732fa494a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = f_emd_IF_dx\n",
    "ivec = i_f_emd_IF_dx\n",
    "\n",
    "table_index = 4\n",
    "varname = 'IMF1 IF(v_{x,fix})'\n",
    "\n",
    "vec = np.array(vec)\n",
    "ivec = np.array(ivec)\n",
    "s_ivec = np.swapaxes(ivec,0,1)\n",
    "s_vec = np.swapaxes(vec,0,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "print(index_fopt60,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][table_index] = varname\n",
    "table[1][table_index] = index_fopt60*500*0.001\n",
    "table[2][table_index] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][table_index] = index_fopt120*500*0.001\n",
    "table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04246af6-8afe-4831-87f5-9873d61a26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### IMF 2 #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457efb61-d2f1-438d-9e19-006203e07a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i_f_emd_IF_dx = []\n",
    "f_emd_IF_dx = []\n",
    "i_f_emd_IA_dx = []\n",
    "f_emd_IA_dx = []\n",
    "\n",
    "for i in range(len(dX)):\n",
    "    if np.sum(Ssaccade[i])>0:\n",
    "        imf = emd.sift.mask_sift(dX[i][Ssaccade[i]==1], max_imfs=4)\n",
    "    else:\n",
    "        imf = emd.sift.mask_sift(dX[i], max_imfs=4)\n",
    "    IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "    \n",
    "    #emd.plotting.plot_imfs(imf[:1000*3, :])\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    # Plot a simple histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 150),color=lcolor);\n",
    "    #peak_IF.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 1],bw_method=0.07)\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram')\n",
    "    plt.xticks(np.arange(0, 500, 100))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "\n",
    "    cummulative_IF = np.cumsum(ghist)/np.sum(ghist)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    # Plot an amplitude-weighted histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 100), weights=IA[:, 2],color=lcolor)\n",
    "    #peak_IA.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 1],bw_method=0.07, weights=np.abs(IA[:, 2]))\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram\\nweighted by IA')\n",
    "    plt.xticks(np.arange(0, 500, 100))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.show()\n",
    "    \n",
    "    cummulative_IA = np.cumsum(ghist)/np.sum(ghist)\n",
    "\n",
    "\n",
    "    i_f_emd_IF_dx.append([])\n",
    "    f_emd_IF_dx.append([])\n",
    "    i_f_emd_IA_dx.append([])\n",
    "    f_emd_IA_dx.append([])\n",
    "    for fff in np.arange(0,1,0.001):\n",
    "        i_f_emd_IF_dx[-1].append(gbins[np.where(cummulative_IF>=fff)[0][0]])\n",
    "        f_emd_IF_dx[-1].append(cummulative_IF[int(len(cummulative_IF)*fff)])\n",
    "        i_f_emd_IA_dx[-1].append(gbins[np.where(cummulative_IA>=fff)[0][0]])\n",
    "        f_emd_IA_dx[-1].append(cummulative_IA[int(len(cummulative_IA)*fff)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eddb8c-48b1-4c33-a800-d2f115b4446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = f_emd_IF_dx\n",
    "ivec = i_f_emd_IF_dx\n",
    "\n",
    "table_index = 5\n",
    "varname = 'IMF2 IF(v_{x,fix})'\n",
    "\n",
    "vec = np.array(vec)\n",
    "ivec = np.array(ivec)\n",
    "s_ivec = np.swapaxes(ivec,0,1)\n",
    "s_vec = np.swapaxes(vec,0,1)\n",
    "\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][table_index] = varname\n",
    "table[1][table_index] = index_fopt60*500*0.001\n",
    "table[2][table_index] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][table_index] = index_fopt120*500*0.001\n",
    "table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b9654-922f-4079-b026-b748319830ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### IMF 3 fix vel #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333235a-e6be-48be-ad27-0c1862669479",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_IF = []\n",
    "peak_IF60 = []\n",
    "peak_IF60plus = []\n",
    "peak_IA60 = []\n",
    "peak_IA60plus = []\n",
    "peak_IA = []\n",
    "\n",
    "i_f_emd_IF_dx = []\n",
    "f_emd_IF_dx = []\n",
    "i_f_emd_IA_dx = []\n",
    "f_emd_IA_dx = []\n",
    "\n",
    "for i in range(len(dX)):\n",
    "    if np.sum(Ssaccade[i])>0:\n",
    "        imf = emd.sift.mask_sift(dX[i][Ssaccade[i]==1], max_imfs=4)\n",
    "    else:\n",
    "        imf = emd.sift.mask_sift(dX[i], max_imfs=4)\n",
    "    IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "    \n",
    "    #emd.plotting.plot_imfs(imf[:1000*3, :])\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    # Plot a simple histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 150),color=lcolor);\n",
    "    #peak_IF.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 2],bw_method=0.07)\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    peak_IF.append( gbins[np.argmax(ghist)])\n",
    "    peak_IF60.append(ghist[60])\n",
    "    peak_IF60plus.append(np.sum(ghist[60:]))\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram')\n",
    "    plt.xticks(np.arange(0, 150, 100))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "\n",
    "    cummulative_IF = np.cumsum(ghist)/np.sum(ghist)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    # Plot an amplitude-weighted histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 100), weights=IA[:, 2],color=lcolor)\n",
    "    #peak_IA.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 2],bw_method=0.07, weights=np.abs(IA[:, 2]))\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    peak_IA.append( gbins[np.argmax(ghist)])\n",
    "    peak_IA60.append(ghist[60])\n",
    "    peak_IA60plus.append(np.sum(ghist[60:]))\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram\\nweighted by IA')\n",
    "    plt.xticks(np.arange(0, 150, 100))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.show()\n",
    "    \n",
    "    cummulative_IA = np.cumsum(ghist)/np.sum(ghist)\n",
    "\n",
    "\n",
    "    i_f_emd_IF_dx.append([])\n",
    "    f_emd_IF_dx.append([])\n",
    "    i_f_emd_IA_dx.append([])\n",
    "    f_emd_IA_dx.append([])\n",
    "    for fff in np.arange(0,1,0.001):\n",
    "        i_f_emd_IF_dx[-1].append(gbins[np.where(cummulative_IF>=fff)[0][0]])\n",
    "        f_emd_IF_dx[-1].append(cummulative_IF[int(len(cummulative_IF)*fff)])\n",
    "        i_f_emd_IA_dx[-1].append(gbins[np.where(cummulative_IA>=fff)[0][0]])\n",
    "        f_emd_IA_dx[-1].append(cummulative_IA[int(len(cummulative_IA)*fff)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1fc78f-90e7-4fad-9a33-c2baa284f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = f_emd_IF_dx\n",
    "ivec = i_f_emd_IF_dx\n",
    "\n",
    "table_index = 6\n",
    "varname = 'IMF3 IF(v_{x,fix})'\n",
    "\n",
    "vec = np.array(vec)\n",
    "ivec = np.array(ivec)\n",
    "s_ivec = np.swapaxes(ivec,0,1)\n",
    "s_vec = np.swapaxes(vec,0,1)\n",
    "\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][table_index] = varname\n",
    "table[1][table_index] = index_fopt60*500*0.001\n",
    "table[2][table_index] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][table_index] = index_fopt120*500*0.001\n",
    "table[4][table_index] = pvals_f[index_fopt120]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63755067-4b72-49a7-92cd-5a8f944b5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# IMF 1 - Y ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c7176-3038-4560-9787-7e854c1d96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "i_f_emd_IF_dy = []\n",
    "f_emd_IF_dy = []\n",
    "i_f_emd_IA_dy = []\n",
    "f_emd_IA_dy = []\n",
    "\n",
    "for i in range(len(dX)):\n",
    "    if np.sum(Ssaccade[i])>0:\n",
    "        imf = emd.sift.mask_sift(dY[i][Ssaccade[i]==1], max_imfs=4)\n",
    "    else:\n",
    "        imf = emd.sift.mask_sift(dY[i], max_imfs=4)\n",
    "    IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "    \n",
    "    #emd.plotting.plot_imfs(imf[:1000*3, :])\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    # Plot a simple histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 150),color=lcolor);\n",
    "    #peak_IF.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 0],bw_method=0.07)\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram')\n",
    "    plt.xticks(np.arange(0, 500, 20))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "\n",
    "    cummulative_IF = np.cumsum(ghist)/np.sum(ghist)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    # Plot an amplitude-weighted histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 100), weights=IA[:, 2],color=lcolor)\n",
    "    #peak_IA.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 0],bw_method=0.07, weights=np.abs(IA[:, 2]))\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram\\nweighted by IA')\n",
    "    plt.xticks(np.arange(0, 500, 20))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.show()\n",
    "    \n",
    "    cummulative_IA = np.cumsum(ghist)/np.sum(ghist)\n",
    "\n",
    "\n",
    "    i_f_emd_IF_dy.append([])\n",
    "    f_emd_IF_dy.append([])\n",
    "    i_f_emd_IA_dy.append([])\n",
    "    f_emd_IA_dy.append([])\n",
    "    for fff in np.arange(0,1,0.001):\n",
    "        i_f_emd_IF_dy[-1].append(gbins[np.where(cummulative_IF>=fff)[0][0]])\n",
    "        f_emd_IF_dy[-1].append(cummulative_IF[int(len(cummulative_IF)*fff)])\n",
    "        i_f_emd_IA_dy[-1].append(gbins[np.where(cummulative_IA>=fff)[0][0]])\n",
    "        f_emd_IA_dy[-1].append(cummulative_IA[int(len(cummulative_IA)*fff)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58780e-3790-4c47-b17b-ebdba5c12715",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = f_emd_IF_dy\n",
    "ivec = i_f_emd_IF_dy\n",
    "\n",
    "table_index = 7\n",
    "varname = 'IMF1 IF(v_{y,fix})'\n",
    "\n",
    "vec = np.array(vec)\n",
    "ivec = np.array(ivec)\n",
    "s_ivec = np.swapaxes(ivec,0,1)\n",
    "s_vec = np.swapaxes(vec,0,1)\n",
    "\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][table_index] = varname\n",
    "table[1][table_index] = index_fopt60*500*0.001\n",
    "table[2][table_index] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][table_index] = index_fopt120*500*0.001\n",
    "table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4c877-c785-40a1-8954-cab08677ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = ['#01204E','#028391','#811638','#FAA968']\n",
    "index = index_fopt60\n",
    "\n",
    "plt.errorbar(np.linspace(0,500,1000),np.mean(vec[group00],axis=0),yerr=0.5*np.std(vec[group00],axis=0)/np.sqrt(np.sum(group00)),c = color_palette[0], label=r'$\\mathcal{P}_{<60}$')\n",
    "plt.errorbar(np.linspace(3,503,1000),np.mean(vec[group10],axis=0),yerr=0.5*np.std(vec[group10],axis=0)/np.sqrt(np.sum(group10)),c = color_palette[1], label=r'$\\mathcal{P}_{>60}$',alpha=1)\n",
    "plt.xlabel(r'$f^{\\prime}$ (Hz)',fontsize = 25)\n",
    "plt.ylabel(r'$\\langle \\text{cdf}_{v_{y,\\text{fix}},S1}(f^{\\prime})\\rangle$',fontsize = 25)\n",
    "plt.vlines(index*500*0.001,0,1,linestyle='--',color='dimgrey')\n",
    "plt.annotate(r'$f^*$',(index*500*0.001+5,0.05),fontsize = 25)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('emd_C_group60.pdf',dpi=300,format=\"pdf\", bbox_inches=\"tight\" )\n",
    "plt.show()\n",
    "\n",
    "index = index_fopt120\n",
    "\n",
    "bins = np.arange(0,0.3,0.001)\n",
    "plt.errorbar(np.linspace(0,500,1000),np.mean(vec[group00],axis=0),yerr=0.2*np.std(vec[group00],axis=0)/np.sqrt(np.sum(group00)),c = color_palette[2], label=r'$\\mathcal{P}_{<120}$')\n",
    "plt.errorbar(np.linspace(3,503,1000),np.mean(vec[group11],axis=0),yerr=0.2*np.std(vec[group11],axis=0)/np.sqrt(np.sum(group11)),c = color_palette[3], label=r'$\\mathcal{P}_{>120}$',alpha=1)\n",
    "plt.vlines(index*500*0.001,0,1,linestyle='--',color='dimgrey')\n",
    "plt.annotate(r'$f^*$',(index*500*0.001+5,0.05),fontsize = 25)\n",
    "plt.xlabel(r'$f^{\\prime}$ (Hz)',fontsize = 25)\n",
    "plt.ylabel(r'$\\langle \\text{cdf}_{v_{y,\\text{fix}},S1}(f^{\\prime})\\rangle$',fontsize = 25)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('emd_C_group120.pdf',dpi=300,format=\"pdf\", bbox_inches=\"tight\" )\n",
    "plt.show()\n",
    "\n",
    "index = index_fopt60\n",
    "color_palette = ['#01204E','#028391','#811638','#FAA968']\n",
    "index = np.argmin(pvals_f)\n",
    "\n",
    "kde00 = stats.gaussian_kde(s_vec[index][group00],bw_method=0.2)\n",
    "kde10 = stats.gaussian_kde(s_vec[index][group10],bw_method=0.2)\n",
    "#kde11 = stats.gaussian_kde(s_f_normed_dy[index][group11==1],bw_method=0.2)\n",
    "bins = np.arange(0.95,1,0.0001)\n",
    "plt.plot(bins,kde00(bins),c = color_palette[0], label=r'$\\mathcal{P}_{<60}$')\n",
    "plt.plot(bins,kde10(bins),c = color_palette[1], label=r'$\\mathcal{P}_{>60}$')\n",
    "plt.ylabel('Density',fontsize = 21)\n",
    "plt.xlabel(r'$\\text{cdf}_{v_{y,\\text{fix}},S1}(f^*=' + str(index_fopt60*500*0.001) +r'\\text{Hz})$',fontsize = 21)\n",
    "plt.tight_layout()\n",
    "#plt.plot(bins,kde11(bins),c = color_palette[2], label=r'$\\mathcal{P}_{>120}$')\n",
    "plt.savefig('emd_group60.png',dpi=300)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "index = index_fopt120\n",
    "color_palette = ['#01204E','#028391','#811638','#FAA968']\n",
    "index = np.argmin(pvals_f)\n",
    "\n",
    "kde11 = stats.gaussian_kde(s_vec[index][group11],bw_method=0.2)\n",
    "kdeN11 = stats.gaussian_kde(s_vec[index][~group11],bw_method=0.2)\n",
    "\n",
    "bins = np.arange(0.95,1,0.0001)\n",
    "plt.plot(bins,kdeN11(bins),c = color_palette[2], label=r'$\\mathcal{P}_{<120}$')\n",
    "plt.plot(bins,kde11(bins),c = color_palette[3], label=r'$\\mathcal{P}_{>120}$')\n",
    "#plt.plot(bins,kde11(bins),c = color_palette[2], label=r'$\\mathcal{P}_{>120}$')\n",
    "plt.ylabel('Density',fontsize = 21)\n",
    "plt.xlabel(r'$\\text{cdf}_{v_{y,\\text{fix}},S1}(f^*=' + str(index_fopt120*500*0.001) +r'\\text{Hz})$',fontsize = 21)\n",
    "plt.tight_layout()\n",
    "plt.savefig('emd_group120.png',dpi=300)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7a53e-d74c-4d8f-80d9-b901f1556e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# IMF 2 - Y ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533380b3-865a-4145-b656-30132cae63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "i_f_emd_IF_dy = []\n",
    "f_emd_IF_dy = []\n",
    "i_f_emd_IA_dy = []\n",
    "f_emd_IA_dy = []\n",
    "\n",
    "for i in range(len(dX)):\n",
    "    if np.sum(Ssaccade[i])>0:\n",
    "        imf = emd.sift.mask_sift(dY[i][Ssaccade[i]==1], max_imfs=4)\n",
    "    else:\n",
    "        imf = emd.sift.mask_sift(dY[i], max_imfs=4)\n",
    "    IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "    \n",
    "    #emd.plotting.plot_imfs(imf[:1000*3, :])\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    # Plot a simple histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 150),color=lcolor);\n",
    "    #peak_IF.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 1],bw_method=0.07)\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram')\n",
    "    plt.xticks(np.arange(0, 500, 20))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "\n",
    "    cummulative_IF = np.cumsum(ghist)/np.sum(ghist)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    # Plot an amplitude-weighted histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 100), weights=IA[:, 2],color=lcolor)\n",
    "    #peak_IA.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 1],bw_method=0.07, weights=np.abs(IA[:, 2]))\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram\\nweighted by IA')\n",
    "    plt.xticks(np.arange(0, 500, 20))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.show()\n",
    "    \n",
    "    cummulative_IA = np.cumsum(ghist)/np.sum(ghist)\n",
    "\n",
    "\n",
    "    i_f_emd_IF_dy.append([])\n",
    "    f_emd_IF_dy.append([])\n",
    "    i_f_emd_IA_dy.append([])\n",
    "    f_emd_IA_dy.append([])\n",
    "    for fff in np.arange(0,1,0.001):\n",
    "        i_f_emd_IF_dy[-1].append(gbins[np.where(cummulative_IF>=fff)[0][0]])\n",
    "        f_emd_IF_dy[-1].append(cummulative_IF[int(len(cummulative_IF)*fff)])\n",
    "        i_f_emd_IA_dy[-1].append(gbins[np.where(cummulative_IA>=fff)[0][0]])\n",
    "        f_emd_IA_dy[-1].append(cummulative_IA[int(len(cummulative_IA)*fff)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10817c48-efae-4217-a6f4-21227a8b0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = f_emd_IF_dy\n",
    "ivec = i_f_emd_IF_dy\n",
    "\n",
    "table_index = 8\n",
    "varname = 'IMF2 IF(v_{y,fix})'\n",
    "\n",
    "vec = np.array(vec)\n",
    "ivec = np.array(ivec)\n",
    "s_ivec = np.swapaxes(ivec,0,1)\n",
    "s_vec = np.swapaxes(vec,0,1)\n",
    "\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][table_index] = varname\n",
    "table[1][table_index] = index_fopt60*500*0.001\n",
    "table[2][table_index] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][table_index] = index_fopt120*500*0.001\n",
    "table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17070140-338e-4717-a2e4-d6d4f513ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## IMF 3 - Y ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fadc948-3a59-463d-8306-3ad026c52b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "i_f_emd_IF_dy = []\n",
    "f_emd_IF_dy = []\n",
    "i_f_emd_IA_dy = []\n",
    "f_emd_IA_dy = []\n",
    "\n",
    "for i in range(len(dX)):\n",
    "    if np.sum(Ssaccade[i])>0:\n",
    "        imf = emd.sift.mask_sift(dY[i][Ssaccade[i]==1], max_imfs=4)\n",
    "    else:\n",
    "        imf = emd.sift.mask_sift(dY[i], max_imfs=4)\n",
    "    IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "    \n",
    "    #emd.plotting.plot_imfs(imf[:1000*3, :])\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    # Plot a simple histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 150),color=lcolor);\n",
    "    #peak_IF.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 2],bw_method=0.07)\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram')\n",
    "    plt.xticks(np.arange(0, 150, 20))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "\n",
    "    cummulative_IF = np.cumsum(ghist)/np.sum(ghist)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    # Plot an amplitude-weighted histogram using frequency bins from 0-20Hz\n",
    "    #a = plt.hist(IF[:, 2], np.linspace(0, 100), weights=IA[:, 2],color=lcolor)\n",
    "    #peak_IA.append(a[1][np.argmax(a[0])])\n",
    "    kde = stats.gaussian_kde(IF[:, 2],bw_method=0.07, weights=np.abs(IA[:, 2]))\n",
    "    gbins = np.arange(0,500,0.5)\n",
    "    ghist = kde(gbins)\n",
    "    plt.plot(gbins,ghist)\n",
    "    plt.grid(True)\n",
    "    plt.title('IF Histogram\\nweighted by IA')\n",
    "    plt.xticks(np.arange(0, 150, 20))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.show()\n",
    "    \n",
    "    cummulative_IA = np.cumsum(ghist)/np.sum(ghist)\n",
    "\n",
    "\n",
    "    i_f_emd_IF_dy.append([])\n",
    "    f_emd_IF_dy.append([])\n",
    "    i_f_emd_IA_dy.append([])\n",
    "    f_emd_IA_dy.append([])\n",
    "    for fff in np.arange(0,1,0.001):\n",
    "        i_f_emd_IF_dy[-1].append(gbins[np.where(cummulative_IF>=fff)[0][0]])\n",
    "        f_emd_IF_dy[-1].append(cummulative_IF[int(len(cummulative_IF)*fff)])\n",
    "        i_f_emd_IA_dy[-1].append(gbins[np.where(cummulative_IA>=fff)[0][0]])\n",
    "        f_emd_IA_dy[-1].append(cummulative_IA[int(len(cummulative_IA)*fff)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceac108-764e-4792-84d8-190974c8f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = f_emd_IF_dy\n",
    "ivec = i_f_emd_IF_dy\n",
    "\n",
    "table_index = 9\n",
    "varname = 'IMF3 IF(v_{y,fix})'\n",
    "\n",
    "vec = np.array(vec)\n",
    "ivec = np.array(ivec)\n",
    "s_ivec = np.swapaxes(ivec,0,1)\n",
    "s_vec = np.swapaxes(vec,0,1)\n",
    "\n",
    "\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "\n",
    "index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "table[0][table_index] = varname\n",
    "table[1][table_index] = index_fopt60*500*0.001\n",
    "table[2][table_index] = pvals_f[index_fopt60]\n",
    "\n",
    "###################### 120 hz #####################\n",
    "pvals_i = []\n",
    "pvals_f = []\n",
    "for i in range(len(s_ivec)):\n",
    "    pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "    pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "\n",
    "index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "\n",
    "selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "#print(index_fopt*500*0.001,min(pvals_f))\n",
    "\n",
    "\n",
    "table[3][table_index] = index_fopt120*500*0.001\n",
    "table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd519b-a333-473a-a57e-6b3182a0e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# IMF 1-3 - v unit X #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd226d2-3fc7-4d4b-bb68-370c060380cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iii in range(3):\n",
    "    l_imf = iii\n",
    "    table_index = 10+iii\n",
    "    varname = 'IMF'+str(iii+1)+' IF(v_{x,unit})'\n",
    "    \n",
    "    i_f_emd_IF_dy = []\n",
    "    f_emd_IF_dy = []\n",
    "    i_f_emd_IA_dy = []\n",
    "    f_emd_IA_dy = []\n",
    "    \n",
    "    for k in range(len(dX)):\n",
    "        if k%10==0:\n",
    "            print(k)\n",
    "        \n",
    "        vunit = np.cos(np.array(angle_single_vector(np.diff(XX[k][::skip_lag]),np.diff(YY[k][::skip_lag])))*math.pi/180)\n",
    "        imf = emd.sift.mask_sift(vunit, max_imfs=4)\n",
    "        IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "        \n",
    "        #emd.plotting.plot_imfs(imf[:1000*3, :])\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        # Plot a simple histogram using frequency bins from 0-20Hz\n",
    "        #a = plt.hist(IF[:, 2], np.linspace(0, 150),color=lcolor);\n",
    "        #peak_IF.append(a[1][np.argmax(a[0])])\n",
    "        kde = stats.gaussian_kde(IF[:, l_imf],bw_method=0.07)\n",
    "        gbins = np.arange(0,500,0.5)\n",
    "        ghist = kde(gbins)\n",
    "        plt.plot(gbins,ghist)\n",
    "        plt.grid(True)\n",
    "        plt.title('IF Histogram')\n",
    "        plt.xticks(np.arange(0, 150, 20))\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "    \n",
    "        cummulative_IF = np.cumsum(ghist)/np.sum(ghist)\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        # Plot an amplitude-weighted histogram using frequency bins from 0-20Hz\n",
    "        #a = plt.hist(IF[:, 2], np.linspace(0, 100), weights=IA[:, 2],color=lcolor)\n",
    "        #peak_IA.append(a[1][np.argmax(a[0])])\n",
    "        kde = stats.gaussian_kde(IF[:, l_imf],bw_method=0.07, weights=np.abs(IA[:, 2]))\n",
    "        gbins = np.arange(0,500,0.5)\n",
    "        ghist = kde(gbins)\n",
    "        plt.plot(gbins,ghist)\n",
    "        plt.grid(True)\n",
    "        plt.title('IF Histogram\\nweighted by IA')\n",
    "        plt.xticks(np.arange(0, 150, 20))\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        cummulative_IA = np.cumsum(ghist)/np.sum(ghist)\n",
    "    \n",
    "    \n",
    "        i_f_emd_IF_dy.append([])\n",
    "        f_emd_IF_dy.append([])\n",
    "        i_f_emd_IA_dy.append([])\n",
    "        f_emd_IA_dy.append([])\n",
    "        for fff in np.arange(0,1,0.001):\n",
    "            i_f_emd_IF_dy[-1].append(gbins[np.where(cummulative_IF>=fff)[0][0]])\n",
    "            f_emd_IF_dy[-1].append(cummulative_IF[int(len(cummulative_IF)*fff)])\n",
    "            i_f_emd_IA_dy[-1].append(gbins[np.where(cummulative_IA>=fff)[0][0]])\n",
    "            f_emd_IA_dy[-1].append(cummulative_IA[int(len(cummulative_IA)*fff)])\n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    vec = f_emd_IF_dy\n",
    "    ivec = i_f_emd_IF_dy\n",
    "    \n",
    "    vec = np.array(vec)\n",
    "    ivec = np.array(ivec)\n",
    "    s_ivec = np.swapaxes(ivec,0,1)\n",
    "    s_vec = np.swapaxes(vec,0,1)\n",
    "    \n",
    "    \n",
    "    pvals_i = []\n",
    "    pvals_f = []\n",
    "    for i in range(len(s_ivec)):\n",
    "        pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "        pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "    \n",
    "    index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "    \n",
    "    selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "    selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "    print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "    #print(index_fopt*500*0.001,min(pvals_f))\n",
    "    \n",
    "    table[0][table_index] = varname\n",
    "    table[1][table_index] = index_fopt60*500*0.001\n",
    "    table[2][table_index] = pvals_f[index_fopt60]\n",
    "    \n",
    "    ###################### 120 hz #####################\n",
    "    pvals_i = []\n",
    "    pvals_f = []\n",
    "    for i in range(len(s_ivec)):\n",
    "        pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "        pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "    \n",
    "    index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "    \n",
    "    selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "    selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "    print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "    #print(index_fopt*500*0.001,min(pvals_f))\n",
    "    \n",
    "    \n",
    "    table[3][table_index] = index_fopt120*500*0.001\n",
    "    table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3ab96-9121-4626-9f86-25de36089fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vec[100][group00],s_vec[100][group10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4accfbc-3281-4a1f-99eb-4ccaf2fc7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb142fc-a745-473c-831a-cf813f18af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95185b-a88c-4443-9ccf-77bf9bcdebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_emd_IF_dy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbcfc2-3165-4bdc-8a2a-603df26ddda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16762294-ddd2-4602-bb3e-11c55f6a6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vec[i][group00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87b771-1f36-4d7a-8e8f-3799aae184a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iii in range(3):\n",
    "    l_imf = iii\n",
    "    table_index = 13+iii\n",
    "    varname = 'IMF'+str(iii+1)+' IF(v_{y,unit})'\n",
    "    \n",
    "    i_f_emd_IF_dy = []\n",
    "    f_emd_IF_dy = []\n",
    "    i_f_emd_IA_dy = []\n",
    "    f_emd_IA_dy = []\n",
    "    \n",
    "    \n",
    "    for k in range(len(dX)):\n",
    "        if k%10==0:\n",
    "            print(k)\n",
    "        vunit = np.sin(np.array(angle_single_vector(np.diff(XX[k][::skip_lag]),np.diff(YY[k][::skip_lag])))*math.pi/180)\n",
    "        imf = emd.sift.mask_sift(vunit, max_imfs=4)\n",
    "        IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "        \n",
    "        #emd.plotting.plot_imfs(imf[:1000*3, :])\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        # Plot a simple histogram using frequency bins from 0-20Hz\n",
    "        #a = plt.hist(IF[:, 2], np.linspace(0, 150),color=lcolor);\n",
    "        #peak_IF.append(a[1][np.argmax(a[0])])\n",
    "        kde = stats.gaussian_kde(IF[:, l_imf],bw_method=0.07)\n",
    "        gbins = np.arange(0,500,0.5)\n",
    "        ghist = kde(gbins)\n",
    "        plt.plot(gbins,ghist)\n",
    "        plt.grid(True)\n",
    "        plt.title('IF Histogram')\n",
    "        plt.xticks(np.arange(0, 150, 20))\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "    \n",
    "        cummulative_IF = np.cumsum(ghist)/np.sum(ghist)\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        # Plot an amplitude-weighted histogram using frequency bins from 0-20Hz\n",
    "        #a = plt.hist(IF[:, 2], np.linspace(0, 100), weights=IA[:, 2],color=lcolor)\n",
    "        #peak_IA.append(a[1][np.argmax(a[0])])\n",
    "        kde = stats.gaussian_kde(IF[:, l_imf],bw_method=0.07, weights=np.abs(IA[:, 2]))\n",
    "        gbins = np.arange(0,500,0.5)\n",
    "        ghist = kde(gbins)\n",
    "        plt.plot(gbins,ghist)\n",
    "        plt.grid(True)\n",
    "        plt.title('IF Histogram\\nweighted by IA')\n",
    "        plt.xticks(np.arange(0, 150, 20))\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        cummulative_IA = np.cumsum(ghist)/np.sum(ghist)\n",
    "    \n",
    "    \n",
    "        i_f_emd_IF_dy.append([])\n",
    "        f_emd_IF_dy.append([])\n",
    "        i_f_emd_IA_dy.append([])\n",
    "        f_emd_IA_dy.append([])\n",
    "        for fff in np.arange(0,1,0.001):\n",
    "            i_f_emd_IF_dy[-1].append(gbins[np.where(cummulative_IF>=fff)[0][0]])\n",
    "            f_emd_IF_dy[-1].append(cummulative_IF[int(len(cummulative_IF)*fff)])\n",
    "            i_f_emd_IA_dy[-1].append(gbins[np.where(cummulative_IA>=fff)[0][0]])\n",
    "            f_emd_IA_dy[-1].append(cummulative_IA[int(len(cummulative_IA)*fff)])\n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    vec = f_emd_IF_dy\n",
    "    ivec = i_f_emd_IF_dy\n",
    "    \n",
    "    vec = np.array(vec)\n",
    "    ivec = np.array(ivec)\n",
    "    s_ivec = np.swapaxes(ivec,0,1)\n",
    "    s_vec = np.swapaxes(vec,0,1)\n",
    "    \n",
    "    \n",
    "    pvals_i = []\n",
    "    pvals_f = []\n",
    "    for i in range(len(s_ivec)):\n",
    "        pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "        pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "    \n",
    "    index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "    \n",
    "    selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "    selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "    print(index_fopt60*500*0.001,pvals_f[index_fopt60])\n",
    "    #print(index_fopt*500*0.001,min(pvals_f))\n",
    "    \n",
    "    table[0][table_index] = varname\n",
    "    table[1][table_index] = index_fopt60*500*0.001\n",
    "    table[2][table_index] = pvals_f[index_fopt60]\n",
    "    \n",
    "    ###################### 120 hz #####################\n",
    "    pvals_i = []\n",
    "    pvals_f = []\n",
    "    for i in range(len(s_ivec)):\n",
    "        pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "        pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "    \n",
    "    index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "    \n",
    "    selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "    selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "    print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "    #print(index_fopt*500*0.001,min(pvals_f))\n",
    "    \n",
    "    \n",
    "    table[3][table_index] = index_fopt120*500*0.001\n",
    "    table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b6468-de9e-4085-a52e-98010487db54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08d5c4-a10a-41ed-bdd7-2418393c3bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe70c2-f9bb-4821-a515-ef6d18fddac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24283bbf-8a2f-40d1-af92-3b4d973f9add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384dd99-4ce6-4191-b3bc-7a6117ef8235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b67076-78c9-49ae-87e2-6a5538c4c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "for i in range(len(table[2])):\n",
    "    table[1][i] = '%.1f' % Decimal(table[1][i])\n",
    "    table[2][i] = '%.2E' % Decimal(table[2][i])\n",
    "    table[3][i] = '%.1f' % Decimal(table[3][i])\n",
    "    table[4][i] = '%.2E' % Decimal(table[4][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba8ef0-5047-44bf-a188-f39382e944f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "table[2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b71d56e9-a7df-4df3-bfdf-4cbd03d460c4",
   "metadata": {},
   "source": [
    "table[1][1:] = 0.1*table[1][1:] \n",
    "table[3][1:] = 0.1*table[3][1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f6605-fee0-407a-84e3-0cc0911d6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table = 1\n",
    "\n",
    "if save_table:\n",
    "    df = pd.DataFrame(np.swapaxes(table[:-1],0,1), columns=['Variable name','Frequency ($\\mathcal{P}_{>60})$','MWU pval  ($\\mathcal{P}_{>60}$)', 'Frequency ($\\mathcal{P}_{>120})$','MWU pval  ($\\mathcal{P}_{>120}$)'])\n",
    "    df.to_csv('table_pvals.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1789a2e-1921-4f13-b265-501ff23782f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee377f0-bd4f-40d2-9e43-eda60a625ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5de2d9-0770-4e57-885f-7e1ad6a54c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4b60f-7156-4dc6-876a-4ea0a1cd84ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd24c1-6825-4dc0-aace-ce58ec05f951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a819730-2f57-4d75-94d7-61741782b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytarget = group10.astype(int)\n",
    "\n",
    "temp_xvariables60 = []\n",
    "temp_xvariables60.append([selected_i_emd_IF_dx60,selected_i_emd_IA_dx60,selected_f_emd_IF_dx60,selected_f_emd_IA_dx60])\n",
    "temp_xvariables60.append([selected_i_emd_IF_dy60,selected_i_emd_IA_dy60,selected_f_emd_IF_dy60,selected_f_emd_IA_dy60])\n",
    "temp_xvariables60.append([selected_i_f_normed_dy60,selected_f_normed_dy60,selected_i_f_normed_dx60,selected_f_normed_dx60])\n",
    "temp_xvariables60.append([selected_i_f_fix_dy60,selected_f_fix_dy60,selected_i_f_fix_dx60,selected_f_fix_dx60])\n",
    "temp_xvariables260 = []\n",
    "for i in range(len(temp_xvariables60)):\n",
    "    for j in range(len(temp_xvariables60[i])):\n",
    "        temp_xvariables260.append(temp_xvariables60[i][j])\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "xvariables60 = np.swapaxes(temp_xvariables260,0,1)\n",
    "\n",
    "\n",
    "#cv_scores = cross_val_score(clf, X_selected, ytarget, cv=5)\n",
    "#print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb0469-3b12-4fcc-bd17-e2d3cbd8b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_list = []\n",
    "for kk in range(len(dX)):\n",
    "    vunit_x = np.cos(np.array(angle_single_vector(np.diff(XX[kk][::skip_lag]),np.diff(YY[kk][::skip_lag])))*math.pi/180)\n",
    "    vunit_y = np.sin(np.array(angle_single_vector(np.diff(XX[kk][::skip_lag]),np.diff(YY[kk][::skip_lag])))*math.pi/180)\n",
    "    vfix_x = dX[kk][Ssaccade[kk]==1]\n",
    "    vfix_y = dY[kk][Ssaccade[kk]==1]\n",
    "    vars_list.append([vunit_x,vunit_y,vfix_x,vfix_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a0a2c-3fa8-41da-9b4a-8ece6a065cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for iii in range(3):\n",
    "    l_imf = iii\n",
    "    table_index = 13+iii\n",
    "    varname = 'IMF'+str(iii+1)+' IF(v_{y,unit})'\n",
    "    \n",
    "    i_f_emd_IF_dy = []\n",
    "    f_emd_IF_dy = []\n",
    "    i_f_emd_IA_dy = []\n",
    "    f_emd_IA_dy = []\n",
    "    \n",
    "    for k in range(len(dX)):\n",
    "        if k%10==0:\n",
    "            print(k)\n",
    "        vunit = np.sin(np.array(angle_single_vector(np.diff(XX[k][::skip_lag]),np.diff(YY[k][::skip_lag])))*math.pi/180)\n",
    "        imf = emd.sift.mask_sift(vunit, max_imfs=4)\n",
    "        IP, IF, IA = emd.spectra.frequency_transform(imf, 1000, 'nht')\n",
    "        \n",
    "        cummulative_IF = np.cumsum(ghist)/np.sum(ghist)\n",
    "        cummulative_IA = np.cumsum(ghist)/np.sum(ghist)\n",
    "    \n",
    "        i_f_emd_IF_dy.append([])\n",
    "        f_emd_IF_dy.append([])\n",
    "        i_f_emd_IA_dy.append([])\n",
    "        f_emd_IA_dy.append([])\n",
    "        for fff in np.arange(0,1,0.001):\n",
    "            i_f_emd_IF_dy[-1].append(gbins[np.where(cummulative_IF>=fff)[0][0]])\n",
    "            f_emd_IF_dy[-1].append(cummulative_IF[int(len(cummulative_IF)*fff)])\n",
    "            i_f_emd_IA_dy[-1].append(gbins[np.where(cummulative_IA>=fff)[0][0]])\n",
    "            f_emd_IA_dy[-1].append(cummulative_IA[int(len(cummulative_IA)*fff)])\n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    vec = f_emd_IF_dy\n",
    "    ivec = i_f_emd_IF_dy\n",
    "    \n",
    "    vec = np.array(vec)\n",
    "    ivec = np.array(ivec)\n",
    "    s_ivec = np.swapaxes(ivec,0,1)\n",
    "    s_vec = np.swapaxes(vec,0,1)\n",
    "    \n",
    "    \n",
    "    pvals_i = []\n",
    "    pvals_f = []\n",
    "    for i in range(len(s_ivec)):\n",
    "        pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group10])[1])\n",
    "        pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group10])[1])\n",
    "    \n",
    "    index_fopt60 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group10],axis=0)))\n",
    "    \n",
    "    selected_i_f_normed_dx60 = s_ivec[index_fopt60]\n",
    "    selected_f_normed_dx60 = s_vec[index_fopt60]\n",
    "\n",
    "    ###################### 120 hz #####################\n",
    "    pvals_i = []\n",
    "    pvals_f = []\n",
    "    for i in range(len(s_ivec)):\n",
    "        pvals_i.append(scipy.stats.mannwhitneyu(s_ivec[i][group00],s_ivec[i][group11])[1])\n",
    "        pvals_f.append(scipy.stats.mannwhitneyu(s_vec[i][group00],s_vec[i][group11])[1])\n",
    "    \n",
    "    index_fopt120 = np.argmax(np.abs(np.mean(vec[group00],axis=0)-np.mean(vec[group11],axis=0)))\n",
    "    \n",
    "    selected_i_f_normed_dx120 = s_ivec[index_fopt120]\n",
    "    selected_f_normed_dx120 = s_vec[index_fopt120]\n",
    "    print(index_fopt120*500*0.001,pvals_f[index_fopt120])\n",
    "    #print(index_fopt*500*0.001,min(pvals_f))\n",
    "    \n",
    "    \n",
    "    table[3][table_index] = index_fopt120*500*0.001\n",
    "    table[4][table_index] = pvals_f[index_fopt120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74ceac-1209-4b54-a882-550c197c30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688260f3-06c3-424a-83f8-dfb3080d3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None,2,4,6,8,10,15, 20, 30],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1,2, 5, 10],\n",
    "    'max_features': [None,1,2,3, 'auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "#param_grid = {\n",
    "#    'criterion': ['gini', 'entropy'],\n",
    "#    'splitter': ['best', 'random'],\n",
    "#    'max_depth': [None, 10, 20, 30],\n",
    "#    'min_samples_split': [2, 10, 20],\n",
    "#    'min_samples_leaf': [1, 5, 10],\n",
    "#    'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "#}\n",
    "scores60 = []\n",
    "#Generate all feature combinations\n",
    "n_features = xvariables60.shape[1]\n",
    "combinations60 = []\n",
    "n_combs = 6\n",
    "for r in range(1, n_combs+1):\n",
    "    combinations60.extend(itertools.combinations(range(n_features), r))\n",
    "\n",
    "#Evaluate each combination\n",
    "clf = DecisionTreeClassifier()\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "for combination in combinations60:\n",
    "    X_subset = xvariables[:, combination]\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_subset, ytarget)\n",
    "    mean_cv_score = np.mean(grid_search.best_score_)\n",
    "    scores60.append(mean_cv_score)\n",
    "    \n",
    "    if mean_cv_score > best_score:\n",
    "        best_score = mean_cv_score\n",
    "        best_combination = combination\n",
    "    print(combination)\n",
    "    print(mean_cv_score)\n",
    "\n",
    "# Step 5: Select the best combination\n",
    "print(best_combination)\n",
    "#print(\"Best feature combination:\", [feature_names[i] for i in best_combination])\n",
    "print(\"Best cross-validation score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f165d4-b4b0-4619-8be3-b1aff89b5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytarget = group11.astype(int)\n",
    "\n",
    "temp_xvariables120 = []\n",
    "temp_xvariables120.append([selected_i_emd_IF_dx120,selected_i_emd_IA_dx120,selected_f_emd_IF_dx120,selected_f_emd_IA_dx120])\n",
    "temp_xvariables120.append([selected_i_emd_IF_dy120,selected_i_emd_IA_dy120,selected_f_emd_IF_dy120,selected_f_emd_IA_dy120])\n",
    "temp_xvariables120.append([selected_i_f_normed_dy120,selected_f_normed_dy120,selected_i_f_normed_dx120,selected_f_normed_dx120])\n",
    "temp_xvariables120.append([selected_i_f_fix_dy120,selected_f_fix_dy120,selected_i_f_fix_dx120,selected_f_fix_dx120])\n",
    "temp_xvariables2120 = []\n",
    "for i in range(len(temp_xvariables120)):\n",
    "    for j in range(len(temp_xvariables120[i])):\n",
    "        temp_xvariables2120.append(temp_xvariables120[i][j])\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "xvariables120 = np.swapaxes(temp_xvariables2120,0,1)\n",
    "\n",
    "\n",
    "#cv_scores = cross_val_score(clf, X_selected, ytarget, cv=5)\n",
    "#print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc10a78-32c7-434a-9ffb-3402e0fc18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None,2,4,6,8,10,15, 20, 30],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1,2, 5, 10],\n",
    "    'max_features': [None,1,2,3, 'auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "#param_grid = {\n",
    "#    'criterion': ['gini', 'entropy'],\n",
    "#    'splitter': ['best', 'random'],\n",
    "#    'max_depth': [None, 10, 20, 30],\n",
    "#    'min_samples_split': [2, 10, 20],\n",
    "#    'min_samples_leaf': [1, 5, 10],\n",
    "#    'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "#}\n",
    "scores120 = []\n",
    "#Generate all feature combinations\n",
    "n_features = xvariables120.shape[1]\n",
    "combinations120 = []\n",
    "n_combs = 6\n",
    "for r in range(1, n_combs+1):\n",
    "    combinations120.extend(itertools.combinations(range(n_features), r))\n",
    "\n",
    "#Evaluate each combination\n",
    "clf = DecisionTreeClassifier()\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "for combination in combinations120:\n",
    "    X_subset = xvariables[:, combination]\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_subset, ytarget)\n",
    "    mean_cv_score = np.mean(grid_search.best_score_)\n",
    "    scores120.append(mean_cv_score)\n",
    "    \n",
    "    if mean_cv_score > best_score:\n",
    "        best_score = mean_cv_score\n",
    "        best_combination = combination\n",
    "    print(combination)\n",
    "    print(mean_cv_score)\n",
    "\n",
    "# Step 5: Select the best combination\n",
    "print(best_combination)\n",
    "#print(\"Best feature combination:\", [feature_names[i] for i in best_combination])\n",
    "print(\"Best cross-validation score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b1a06e-5593-465c-8ecb-0a9f2d97d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216adb29-5477-44c4-b0ce-72dfd787b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array(scores)>0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d54274-dc7a-4dbc-a5dc-b00cde769317",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-1,3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559044b2-d697-4efa-9ae9-fd4b821452c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': np.logspace(-1,3,10),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'shrinking': [True, False]\n",
    "}\n",
    "param_grid = {\n",
    "    'C': [0.1,1,5,10,25,50,100],\n",
    "    'kernel': ['linear', 'sigmoid']\n",
    "}\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "#Generate all feature combinations\n",
    "n_features = xvariables.shape[1]\n",
    "combinations = []\n",
    "n_combs = 3\n",
    "for r in range(1, n_combs+1):\n",
    "    combinations.extend(itertools.combinations(range(n_features), r))\n",
    "\n",
    "#Evaluate each combination\n",
    "clf = SVC()\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "for combination in combinations:\n",
    "    X_subset = xvariables[:, combination]\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_subset, ytarget)\n",
    "    mean_cv_score = np.mean(grid_search.best_score_)\n",
    "    \n",
    "    if mean_cv_score > best_score:\n",
    "        best_score = mean_cv_score\n",
    "        best_combination = combination\n",
    "    print(combination)\n",
    "    print(mean_cv_score)\n",
    "\n",
    "# Step 5: Select the best combination\n",
    "print(best_combination)\n",
    "#print(\"Best feature combination:\", [feature_names[i] for i in best_combination])\n",
    "print(\"Best cross-validation score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077ec5c-284c-4817-9b61-14f633481d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cd883-1641-435d-9afd-eef9181750cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386c9f6-dd07-4930-bd8c-8d51dea131c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'max_features': ['auto', 3,4,5,'sqrt', 'log2']\n",
    "}\n",
    "param_grid = {\n",
    "    'n_estimators': [100]\n",
    "}\n",
    "\n",
    "#Generate all feature combinations\n",
    "n_features = xvariables.shape[1]\n",
    "combinations = []\n",
    "n_combs = 4\n",
    "for r in range(3, n_combs+1):\n",
    "    combinations.extend(itertools.combinations(range(n_features), r))\n",
    "\n",
    "\n",
    "#Evaluate each combination\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "for combination in combinations:\n",
    "    X_subset = xvariables[:, combination]\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_subset, ytarget)\n",
    "    mean_cv_score = np.mean(grid_search.best_score_)\n",
    "    \n",
    "    if mean_cv_score > best_score:\n",
    "        best_score = mean_cv_score\n",
    "        best_combination = combination\n",
    "    print(combination)\n",
    "    print(mean_cv_score)\n",
    "\n",
    "# Step 5: Select the best combination\n",
    "print(best_combination)\n",
    "#print(\"Best feature combination:\", [feature_names[i] for i in best_combination])\n",
    "print(\"Best cross-validation score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e5c71-540b-4d56-9eec-caca9d411a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32ea86-e6ca-481f-9645-7f6305a23765",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b723811-73a4-4faa-ad7a-09f42e6991d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_IF_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6fe68-154f-4ffd-985e-d60594ce7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_f_emd_IF_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc58bb4-6d63-473d-ba0a-bbe305317b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce127f1-001b-4e0b-8d85-cfa23a348bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "IA[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cebad1-e6c4-4ec8-9c07-442e7d6b75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_peak = []\n",
    "pvals_60 = []\n",
    "pvals_60_plus = []\n",
    "\n",
    "print(scipy.stats.mannwhitneyu(peak_IF[sepa:],peak_IF[:sepa])[1])\n",
    "print(scipy.stats.mannwhitneyu(peak_IF60[sepa:],peak_IF60[:sepa])[1])\n",
    "print(scipy.stats.mannwhitneyu(peak_IF60plus[sepa:],peak_IF60plus[:sepa])[1])\n",
    "print(scipy.stats.mannwhitneyu(peak_IA[sepa:],peak_IA[:sepa])[1])\n",
    "print(scipy.stats.mannwhitneyu(peak_IA60[sepa:],peak_IA60[:sepa])[1])\n",
    "print(scipy.stats.mannwhitneyu(peak_IA60plus[sepa:],peak_IA60plus[:sepa])[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfde582-7530-450f-9596-d5f6803baeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(peak_IF60[sepa:],np.arange(0.005,0.02,0.0005),density = True)\n",
    "plt.hist(peak_IF60[:sepa],np.arange(0.005,0.02,0.0005),density = True,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da50974-213d-41a5-ab58-e5cb5744dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(peak_IA60plus[sepa:],np.arange(0.05,0.2,0.005),density = True)\n",
    "plt.hist(peak_IA60plus[:sepa],np.arange(0.05,0.2,0.005),density = True,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735e823-2e18-4ac7-9fe2-157c50626ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(peak_IA60plus[sepa:],density = True)\n",
    "plt.hist(peak_IA60plus[:sepa],density = True,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40ecab-0908-4a02-93c9-95409a776dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c =  plt.hist(IF[:, 2], np.linspace(0, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afd875-7adc-4a3b-b18c-b13573e3d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1][np.argmax(a[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8318c03-e258-4bd9-9bbd-799287f2381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277256d-b13a-4459-bb83-56284e1d3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0, 150)[np.argmax(IF[:, 2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da31d0f-55f0-4741-8a24-ccae8f1043e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(IF[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45271b-960c-4f20-8876-0c224a04b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, spectrum = emd.spectra.hilberthuang(IF, IA, (0, 100, 25))\n",
    "\n",
    "freq_edges, freq_centres = emd.spectra.define_hist_bins(0, 100, 128, 'linear')\n",
    "\n",
    "# Amplitude weighted HHT per IMF\n",
    "f, spec_weighted = emd.spectra.hilberthuang(IF, IA, freq_edges, sum_imfs=False)\n",
    "\n",
    "# Unweighted HHT per IMF - we replace the instantaneous amplitude values with ones\n",
    "f, spec_unweighted = emd.spectra.hilberthuang(IF, np.ones_like(IA), freq_edges, sum_imfs=False)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.subplot(121)\n",
    "plt.plot(freq_centres, spec_unweighted)\n",
    "plt.xticks(np.arange(10)*10)\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('unweighted\\nHilbert-Huang Transform')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(freq_centres, spec_weighted)\n",
    "plt.xticks(np.arange(10)*10)\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power')\n",
    "plt.title('IA-weighted\\nHilbert-Huang Transform')\n",
    "plt.legend(['IMF-1', 'IMF-2', 'IMF-3', 'IMF-4', 'IMF-5'], frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062fb47-4dec-46d7-b961-b8d69fdb5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "IP, IF, IA = emd.spectra.frequency_transform(imf, sample_rate, 'nht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95806a7-2a33-409b-bc54-cd1c38d4c3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7653f38-0794-4834-8e57-80ad7ca4c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies[np.where(np.cumsum(moving_averages)/np.sum(moving_averages)>0.8)[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6b274-8d64-47dc-a5ec-e894f478a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy = []\n",
    "N = 500\n",
    "skip_lag = 3\n",
    "frequency = 1000/skip_lag\n",
    "\n",
    "for k in range(len(dX)):\n",
    "    norm_dx = np.cos(np.array(angle_single_vector(np.diff(XX[k][::skip_lag]),np.diff(YY[k][::skip_lag])))*math.pi/180)\n",
    "    l_P_S = power_spectrum(norm_dx,frequency,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    #plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))\n",
    "\n",
    "    norm_dy = np.sin(np.array(angle_single_vector(np.diff(XX[k][::skip_lag]),np.diff(YY[k][::skip_lag])))*math.pi/180)\n",
    "    l_P_S = power_spectrum(norm_dy,frequency,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4cc80-ed0e-483d-9cda-ca8c208ecaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338ba17-d392-4549-ae47-1cc049b50655",
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy = []\n",
    "N = 1000\n",
    "skip_lag = 4\n",
    "frequency = 1000/skip_lag\n",
    "\n",
    "for k in range(len(dX)):\n",
    "    norm_dx = np.cos(np.array(angle_single_vector(np.diff(XX[k][int(len(XX[k])/2)::skip_lag]),np.diff(YY[k][int(len(YY[k])/2)::skip_lag])))*math.pi/180)\n",
    "    l_P_S = power_spectrum(norm_dx,frequency,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))\n",
    "\n",
    "\n",
    "    norm_dx = np.cos(np.array(angle_single_vector(np.diff(XX[k][:int(len(XX[k])/2):skip_lag]),np.diff(YY[k][:int(len(XX[k])/2):skip_lag])))*math.pi/180)\n",
    "    l_P_S = power_spectrum(norm_dx,frequency,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "    #plt.plot(frequencies,moving_averages)\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "\n",
    "    #plt.vlines(15, min(np.log(moving_averages)), max(np.log(moving_averages)), colors='k', linestyles='solid')\n",
    "    #plt.vlines(30, min(np.log(moving_averages)), max(np.log(moving_averages)), colors='k', linestyles='solid')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301bd641-da88-4371-a698-42f8f1be38bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy = []\n",
    "N = 100\n",
    "frequency = 1000\n",
    "for k in range(len(dX)):\n",
    "    l_P_S = power_spectrum(dX[k],1000,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,moving_averages)\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))\n",
    "      \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50208cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy = []\n",
    "N = 100\n",
    "frequency = 1000\n",
    "for k in range(len(dX)):\n",
    "    l_P_S = power_spectrum(dX[k],1000,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7261f-c605-4c7b-8584-62a53860d325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy = []\n",
    "N = 100\n",
    "frequency = 1000\n",
    "for k in range(len(dY)):\n",
    "    l_P_S = power_spectrum(dY[k],1000,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies,np.log(moving_averages))\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy = []\n",
    "N = 100\n",
    "frequency = 1000\n",
    "for k in range(len(dX)):\n",
    "    l_P_S = power_spectrum(dX[k],1000,False,'')\n",
    "    # Program to calculate moving average\n",
    "\n",
    "    frequencies = np.linspace(0, frequency/2, len(l_P_S)-N+1)\n",
    "    \n",
    "    moving_averages =  np.convolve(l_P_S, np.ones(N)/N, mode='valid')\n",
    "\n",
    "        \n",
    "    plt.plot(frequencies[:int(len(frequencies)/10)],moving_averages[:int(len(frequencies)/10)])\n",
    "\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.xlabel(\"frequency (HZ)\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    Energy.append(np.sum(l_P_S))\n",
    "      \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log2(Energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc66a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.fftpack\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def centers(edges):\n",
    "        return( edges[:-1] + np.diff(edges[:2])/2)\n",
    "def fft_2d(lvector1, lvector2, nbins,graph):\n",
    "    \n",
    "    histogram, xedges, yedges = np.histogram2d(lvector1, lvector2, bins=nbins)\n",
    "    # Figure out centers of bins\n",
    "    x_centers = centers(xedges)\n",
    "    y_centers = centers(yedges)\n",
    "    \n",
    "    #\"\"\"CREATING REAL AND MOMENTUM SPACES GRIDS\"\"\"\n",
    "    N_x,N_y = len(histogram),len(histogram)\n",
    "    range_x, range_y = np.arange(N_x), np.arange(N_y)\n",
    "    \n",
    "    dk_x, dk_y = np.pi / np.max(x_centers), np.pi / np.max(y_centers)\n",
    "# momentum space grid vectors, shifted to center for zero frequency\n",
    "    k_xv, k_yv = dk_x * np.append(range_x[:N_x//2], -range_x[N_x//2:0:-1]), \\\n",
    "             dk_y * np.append(range_y[:N_y//2], -range_y[N_y//2:0:-1])\n",
    "# create real and momentum spaces grids\n",
    "    x, y = np.meshgrid(x_centers, y_centers, sparse=False, indexing='ij')\n",
    "    kx, ky = np.meshgrid(k_xv, k_yv, sparse=False, indexing='ij')\n",
    "    F = scipy.fftpack.fft2(histogram)\n",
    "    if graph:\n",
    "        fig = plt.figure()\n",
    "        ax = Axes3D(fig)\n",
    "        surf = ax.plot_surface(x, y, np.abs(histogram), cmap='viridis')\n",
    "        plt.show()\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = Axes3D(fig)\n",
    "        surf = ax.plot_surface(x, y, np.log10(np.abs(histogram)), cmap='viridis')\n",
    "        plt.show()\n",
    "         # for other plots I changed to\n",
    "#        fig2 = plt.figure()\n",
    "#        ax2 =Axes3D(fig2)\n",
    "#        surf = ax2.plot_surface(kx, ky, np.abs(F)*dx*dy, cmap='viridis')\n",
    "#        plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_2d(dX[0],dY[0],30,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2113b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(dX)):\n",
    "    corr_OG = []\n",
    "    temp_ldx = np.diff(X[k][saccade[k]==1])\n",
    "    for i in range(1,50):\n",
    "        #corr_OG.append( np.corrcoef(np.hstack(d_X_num_clean_lag[0][7])[i:],np.hstack(d_X_num_clean_lag[0][7])[:-i])[1,0])\n",
    "        corr_OG.append( np.corrcoef(temp_ldx[i:],temp_ldx[:-i])[1,0])\n",
    "\n",
    "    plt.plot(np.arange(1,50),corr_OG)\n",
    "    plt.ylim(1.1*min(-0.15,min(corr_OG)),1.1*max(0.15,max(corr_OG)))\n",
    "    plt.hlines([0],1,50,linestyles='--',color='k') \n",
    "    plt.xticks(np.arange(1,50,5))\n",
    "    plt.title('Eye-tracker data: horizontal displacements')\n",
    "    plt.xlabel('lag')\n",
    "    plt.ylabel('correlation coefficient')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aaf4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0307b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "############################# #1 - one variable ########################\n",
    "\n",
    "\n",
    "\n",
    "def frequency_matrix_2D(d__ss,threshold,normalized):\n",
    "    d__ss = np.array(d__ss)\n",
    "    d__ss = (d__ss-min(d__ss))/((max(d__ss)-min(d__ss))*1.000001)\n",
    "    binary_vector = np.array(d__ss>threshold).astype(int)\n",
    "    matrix = np.histogram2d(binary_vector[1:], binary_vector[:-1],[0,1,2])[0]\n",
    "    if normalized:\n",
    "        for j in range(2):\n",
    "            matrix[j,:] = matrix[j,:]/matrix.sum(axis=1)[j]\n",
    "    return( matrix)\n",
    "    \n",
    "\n",
    "        \n",
    "                    \n",
    "    \n",
    "        \n",
    "\n",
    "#in threshold array, 0 corresponds to the minimum of the vector while 1 is the maximum of the vector\n",
    "def form_groups(vector,threshold_array,graph,x_label,title,x_axis_format):\n",
    "    \n",
    "    detectionfisher=[]\n",
    "    detection=[]\n",
    "    for i in threshold_array:\n",
    "        matrix = frequency_matrix_2D(vector,i,False)\n",
    "        detectionfisher.append(np.log(scipy.stats.fisher_exact(matrix)[1]))\n",
    "        p = matrix.sum(axis=1)[0]/matrix.sum()\n",
    "        if p==1 or p==0:\n",
    "            detection.append(1)\n",
    "        else:\n",
    "            detection.append((matrix[1][0]) / (matrix.sum() *(p*(1-p))))\n",
    "\n",
    "    \n",
    "    minim = min(vector)\n",
    "    diff = max(vector) - minim\n",
    "    min_k = np.argmin(detection)*diff + minim\n",
    "    min_fisher = np.argmin(detectionfisher)*diff + minim\n",
    "    \n",
    "    if graph:\n",
    "\n",
    "        xticks_labels = [x_axis_format % (minim + diff*pipi) for pipi in threshold_array]\n",
    "        \n",
    "        \n",
    "        plt.plot(detection)\n",
    "        plt.xlabel(x_label)\n",
    "        #plt.title(title +\"k min = \" +str(round(min_k,5)))\n",
    "        plt.title(title)\n",
    "        if len(threshold_array) > 40:\n",
    "            plt.xticks(np.arange(len(threshold_array))[::int(len(threshold_array)/10)],xticks_labels[::int(len(threshold_array)/10)])\n",
    "        else:\n",
    "            plt.xticks(np.arange(len(threshold_array))[::4],xticks_labels[::4])\n",
    "        plt.ylabel(\"k\")\n",
    "        plt.savefig(\"group_detection - k \" + x_label + title+\".png\",dpi = 500)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        plt.plot(detectionfisher)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.title(title)\n",
    "        if len(threshold_array) > 40:\n",
    "            plt.xticks(np.arange(len(threshold_array))[::int(len(threshold_array)/10)],xticks_labels[::int(len(threshold_array)/10)])\n",
    "        else:\n",
    "            plt.xticks(np.arange(len(threshold_array))[::4],xticks_labels[::4])\n",
    "        plt.ylabel(\"log-fisher exact test\")\n",
    "        plt.savefig(\"group_detection - log-fisher \" + x_label +title+ \".png\",dpi = 500)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "   \n",
    "    return(detection, detectionfisher, min_k,min_fisher)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def real_k_and_fisher(binary_vector):\n",
    "    matrix = np.zeros((2,2))\n",
    "    for i in range(len(binary_vector)-1):\n",
    "        matrix[int(binary_vector[i])][int(binary_vector[i+1])] +=1\n",
    "    \n",
    "    detectionfisher=[]\n",
    "    detection=[]\n",
    "\n",
    "    detectionfisher.append(np.log(scipy.stats.fisher_exact(matrix)[1]))\n",
    "    p = matrix.sum(axis=1)[0]/matrix.sum()\n",
    "    detection.append((matrix[1][0])/(matrix.sum()*(p*(1-p))))\n",
    "\n",
    "    return( matrix,detection, detectionfisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vS_tot = np.hstack(dS) / np.hstack(dT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldetection, ldetectionfisher, lkmin, lfishermin = form_groups(vS_tot,np.arange(1,25,0.25)/max(vS_tot),True,'v','title','%.2f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "vS_tot = np.hstack(dS) / np.hstack(dT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_list = []\n",
    "kmin_list = []\n",
    "for i in range(len(dS)):\n",
    "    #lc_vS = np.array(dS[i]) / np.array(dT[i])\n",
    "    lc_vS = np.array(dS[i]) \n",
    "    raw_threshold_array = np.arange(1,30,0.5)\n",
    "    threshold_array = raw_threshold_array / max(lc_vS)\n",
    "    ldetection, ldetectionfisher, lkmin, lfishermin = form_groups(lc_vS,threshold_array,True,'v','title','%.2f')\n",
    "    detection_list.append(ldetection)\n",
    "    kmin_list.append(lkmin)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "N=5\n",
    "for i in range(len(detection_list)):\n",
    "    print(i)\n",
    "    lc_smooth_det = np.convolve(detection_list[i], np.ones(N), 'valid') / N\n",
    "    plt.plot(lc_smooth_det)\n",
    "    minima = scipy.signal.argrelextrema(lc_smooth_det, np.less)\n",
    "    plt.vlines(minima,0,0.3,color='k')\n",
    "    print(scipy.signal.argrelextrema(lc_smooth_det, np.less))\n",
    "    plt.show()\n",
    "    #plt.plot(np.diff(lc_smooth_det))\n",
    "    #plt.vlines(minima,0,0.3,color='k')\n",
    "    #plt.show()\n",
    "    #plt.plot(np.diff(np.diff(lc_smooth_det)))\n",
    "    #plt.vlines(minima,0,0.3,color='k')\n",
    "    #plt.show()\n",
    "    \n",
    "included = [0,2,4,5,6,7,8,9,10,11,12,13,14,16,18,19,20]    \n",
    "excluded = [1,3,15,17,21,22]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "N=5\n",
    "for i in range(len(detection_list)):\n",
    "    print(i)\n",
    "    lc_smooth_det = np.convolve(detection_list[i], np.ones(N), 'valid') / N\n",
    "    plt.plot(lc_smooth_det)\n",
    "    minima = scipy.signal.argrelextrema(lc_smooth_det, np.less)\n",
    "    plt.vlines(minima,0,0.3,color='k')\n",
    "    print(scipy.signal.argrelextrema(lc_smooth_det, np.less))\n",
    "    plt.show()\n",
    "    #plt.plot(np.diff(lc_smooth_det))\n",
    "    #plt.vlines(minima,0,0.3,color='k')\n",
    "    #plt.show()\n",
    "    #plt.plot(np.diff(np.diff(lc_smooth_det)))\n",
    "    #plt.vlines(minima,0,0.3,color='k')\n",
    "    #plt.show()\n",
    "    \n",
    "included = [0,2,4,5,6,7,8,9,10,11,12,13,14,16,18,19,20]    \n",
    "excluded = [1,3,15,17,21,22]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "N=5\n",
    "included = [0,2,4,5,6,7,8,9,10,11,12,13,14,16,18,19,20]    \n",
    "excluded = [1,3,15,17,21,22]  \n",
    "min_first_min = 10 #first minimum should be lower than this number.\n",
    "gminima =[]\n",
    "for i in range(len(detection_list)):\n",
    "    if i in included:\n",
    "        print(i)\n",
    "        lc_smooth_det = np.convolve(detection_list[i], np.ones(N), 'valid') / N\n",
    "        temp_minima = scipy.signal.argrelextrema(lc_smooth_det, np.less)[0]\n",
    "        gminima.append([])\n",
    "        if temp_minima[0]>min_first_min:\n",
    "            gminima[-1].append(0)\n",
    "            gminima[-1].append(temp_minima[0])\n",
    "        else: \n",
    "            gminima[-1].append(temp_minima[0])\n",
    "            if temp_minima[1] <10:\n",
    "                gminima[-1].append(temp_minima[2])\n",
    "            else:\n",
    "                gminima[-1].append(temp_minima[1])\n",
    "            \n",
    "        plt.plot(lc_smooth_det)\n",
    "        plt.vlines(gminima[-1],0,0.3,color='k')\n",
    "        print(scipy.signal.argrelextrema(lc_smooth_det, np.less))\n",
    "        #plt.savefig()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d544d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 2 types of movement ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229975fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "est_sacc_mu = []\n",
    "est_sacc_sigma = []\n",
    "est_fix_mu = []\n",
    "est_fix_sigma = []\n",
    "est_lambda_B_list = [] #transition saccade to fix \n",
    "est_lambda_D_list = [] # vice versa\n",
    "guess_logdS_fix_list = []\n",
    "guess_logdS_sacc_list = []\n",
    "vS = []\n",
    "for i in range(len(included)):\n",
    "    vS.append(np.array(dS[included[i]]) / np.array(dT[included[i]]))\n",
    "for i in range(len(included)):\n",
    "    lthreshold = raw_threshold_array[gminima[i][0]]\n",
    "    d__ss = vS[i]\n",
    "    binary_vector = np.array([max(min(int(i-lthreshold+1),1),0) for i in d__ss])\n",
    "    guess_dS_fix = vS[i][binary_vector==0]\n",
    "    guess_dS_sacc = vS[i][binary_vector==1]\n",
    "    guess_logdS_fix = np.log(guess_dS_fix[guess_dS_fix>0])\n",
    "    guess_logdS_fix_list.append(guess_logdS_fix)\n",
    "    guess_logdS_sacc = np.log(guess_dS_sacc[guess_dS_sacc>0])\n",
    "    guess_logdS_sacc_list.append(guess_logdS_sacc)\n",
    "    est_fix_mu.append(np.mean(guess_logdS_fix))\n",
    "    est_fix_sigma.append(np.std(guess_logdS_fix))\n",
    "    est_sacc_mu.append(np.mean(guess_logdS_sacc))\n",
    "    est_sacc_sigma.append(np.std(guess_logdS_sacc))\n",
    "    Nfix = len(binary_vector) - np.sum(binary_vector)\n",
    "    Nsacc = np.sum(binary_vector)\n",
    "    Ntransi = int(np.sum(np.abs(binary_vector[1:] - binary_vector[:-1]))/2)\n",
    "    est_lambda_B_list.append(-np.log(Ntransi / Nsacc))\n",
    "    est_lambda_D_list.append(-np.log(Ntransi / Nfix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b96930",
   "metadata": {},
   "outputs": [],
   "source": [
    "gminima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate import *\n",
    "HMM_est_sacc_mu = []\n",
    "HMM_est_sacc_sigma = []\n",
    "HMM_est_fix_mu = []\n",
    "HMM_est_fix_sigma = []\n",
    "HMM_est_D_list = []\n",
    "HMM_est_lambda_B_list = []\n",
    "HMM_est_lambda_D_list = []\n",
    "HMM_logdS_fix_list = []\n",
    "HMM_logdS_sacc_list = []\n",
    "\n",
    "\n",
    "for i in range(len(included)-1):\n",
    "    print(i)\n",
    "    prbd = np.exp(-est_lambda_B_list[i])\n",
    "    prbb = 1-prbd\n",
    "    prdb = np.exp(-est_lambda_D_list[i])\n",
    "    prdd = 1-prdb\n",
    "    s1 = State(LogNormalDistribution(est_fix_mu[i], est_fix_sigma[i])) #fixations\n",
    "    s2 = State(LogNormalDistribution(est_sacc_mu[i], est_sacc_sigma[i])) #saccades\n",
    "    model = HiddenMarkovModel()\n",
    "    model.add_states(s1, s2)\n",
    "    model.add_transition(model.start, s1, 0.5)\n",
    "    model.add_transition(model.start, s2, 0.5)\n",
    "    model.add_transition(s1, s1, prdd)\n",
    "    model.add_transition(s1, s2, prdb)\n",
    "    model.add_transition(s2, s1, prbd)\n",
    "    model.add_transition(s2, s2, prbb)\n",
    "    model.add_transition(s2, model.end, 0.5*len(vS[i][vS[i]>0]))\n",
    "    model.add_transition(s1, model.end, 0.5*len(vS[i][vS[i]>0]))\n",
    "    model.bake()\n",
    "    model.fit( [vS[i][vS[i]>0]] )\n",
    "    temp_vel = vS[i][vS[i]>0]\n",
    "    \n",
    "    s1_mu = model.states[0].distribution.parameters[0]\n",
    "    s2_mu = model.states[1].distribution.parameters[0]\n",
    "    \n",
    "    if s2_mu > s1_mu:\n",
    "        HMM_est_fix_mu.append(model.states[0].distribution.parameters[0])\n",
    "        HMM_est_fix_sigma.append(model.states[0].distribution.parameters[1])\n",
    "        HMM_est_sacc_mu.append(model.states[1].distribution.parameters[0])\n",
    "        HMM_est_sacc_sigma.append(model.states[1].distribution.parameters[1])\n",
    "        HMM_bin_vec = np.swapaxes(model.viterbi(vS[i][vS[i]>0])[1],0,1)[0][1:-1]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        HMM_est_fix_mu.append(model.states[1].distribution.parameters[0])\n",
    "        HMM_est_fix_sigma.append(model.states[1].distribution.parameters[1])\n",
    "        HMM_est_sacc_mu.append(model.states[0].distribution.parameters[0])\n",
    "        HMM_est_sacc_sigma.append(model.states[0].distribution.parameters[1])\n",
    "        HMM_bin_vec = np.swapaxes(model.viterbi(vS[i][vS[i]>0])[1],0,1)[0][1:-1]\n",
    "        HMM_bin_vec = np.abs(np.array(HMM_bin_vec) - np.ones(len(HMM_bin_vec)))\n",
    "\n",
    "    Nfix = len(HMM_bin_vec) - np.sum(HMM_bin_vec)\n",
    "    Nsacc = np.sum(HMM_bin_vec)\n",
    "    Ntransi = int(np.sum(np.abs(HMM_bin_vec[1:] - HMM_bin_vec[:-1]))/2)\n",
    "    HMM_est_lambda_B_list.append(-np.log(1 - Ntransi / Nsacc))\n",
    "    HMM_est_lambda_D_list.append(-np.log(1 - Ntransi / Nfix))\n",
    "    HMM_logdS_fix_list.append(temp_vel[HMM_bin_vec==0])\n",
    "    HMM_logdS_sacc_list.append(temp_vel[HMM_bin_vec==1])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "66b659b0",
   "metadata": {},
   "source": [
    "for i in range(len(included)):\n",
    "    plt.hist(np.log(HMM_logdS_fix_list[i]),np.arange(-2,3,0.2),density=True);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "153c0733",
   "metadata": {},
   "source": [
    "for i in range(len(included)):\n",
    "    plt.hist(np.log(HMM_logdS_sacc_list[i]),np.arange(-2,3,0.2),density=True);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d06865",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(included)):\n",
    "    plt.hist(np.log(HMM_logdS_fix_list[i]),np.arange(-2,4,0.2),density=False);\n",
    "    plt.hist(np.log(HMM_logdS_sacc_list[i]),np.arange(-2,4,0.2),density=False,alpha=0.5);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ac999",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(included)):\n",
    "    plt.hist(np.log(vS[included[i]][vS[included[i]]>0] ),np.arange(-2,3,0.2),density=True);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4d788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a673427",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 3 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "included = [0,2,4,5,6,7,8,9,10,11,12,13,14,16,18,19,20]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "est_pliss_mu = []\n",
    "est_pliss_sigma = []\n",
    "est_sacc_mu = []\n",
    "est_sacc_sigma = []\n",
    "est_fix_mu = []\n",
    "est_fix_sigma = []\n",
    "\n",
    "guess_logdS_fix_list = []\n",
    "guess_logdS_sacc_list = []\n",
    "guess_logdS_pliss_list = []\n",
    "est_lambda_DB_list= []\n",
    "est_lambda_DP_list= []\n",
    "est_lambda_BD_list= []\n",
    "est_lambda_BP_list= []\n",
    "est_lambda_PD_list= []\n",
    "est_lambda_PB_list= []\n",
    "bin_vec_vec = []\n",
    "for i in range(len(gminima)):\n",
    "    lthreshold = raw_threshold_array[gminima[i][0]]\n",
    "    lthreshold2 = raw_threshold_array[gminima[i][1]]\n",
    "    d__ss = vS[i]\n",
    "    binary_vector1 = np.array([max(min(int(i-lthreshold+1),1),0) for i in d__ss])\n",
    "    binary_vector2 = np.array([max(min(int(i-lthreshold2+1),1),0) for i in d__ss])\n",
    "    binary_vector = binary_vector1 + binary_vector2\n",
    "    guess_dS_fix = vS[i][binary_vector==0]\n",
    "    guess_dS_sacc = vS[i][binary_vector==1]\n",
    "    guess_dS_pliss = vS[i][binary_vector==2]\n",
    "    \n",
    "    guess_logdS_fix = np.log(guess_dS_fix[guess_dS_fix>0])\n",
    "    guess_logdS_fix_list.append(guess_logdS_fix)\n",
    "    guess_logdS_sacc = np.log(guess_dS_sacc[guess_dS_sacc>0])\n",
    "    guess_logdS_sacc_list.append(guess_logdS_sacc)\n",
    "    guess_logdS_pliss = np.log(guess_dS_pliss[guess_dS_pliss>0])\n",
    "    guess_logdS_pliss_list.append(guess_dS_pliss)\n",
    "    \n",
    "    est_fix_mu.append(np.mean(guess_logdS_fix))\n",
    "    est_fix_sigma.append(np.std(guess_logdS_fix))\n",
    "    est_sacc_mu.append(np.mean(guess_logdS_sacc))\n",
    "    est_sacc_sigma.append(np.std(guess_logdS_sacc))\n",
    "    est_pliss_mu.append(np.mean(guess_logdS_pliss))\n",
    "    est_pliss_sigma.append(np.std(guess_logdS_pliss))\n",
    "    bin_vec_vec.append(binary_vector)\n",
    "    \n",
    "    bin_vec_sq = binary_vector**2\n",
    "    diff_bin_vec_sq = binary_vector[1:]**2 - binary_vector[:-1]**2\n",
    "    \n",
    "    \n",
    "    Nfix = np.sum(binary_vector==0)\n",
    "    Nsacc = np.sum(binary_vector==1)\n",
    "    Npliss = np.sum(binary_vector==2)\n",
    "    \n",
    "    Nfixsacc =  np.sum(diff_bin_vec_sq==1)\n",
    "    Nfixpliss = np.sum(diff_bin_vec_sq==4)\n",
    "    Nsaccfix = np.sum(diff_bin_vec_sq==-1)\n",
    "    Nsaccpliss = np.sum(diff_bin_vec_sq==3)\n",
    "    Nplissfix = np.sum(diff_bin_vec_sq==-4)\n",
    "    Nplisssacc= np.sum(diff_bin_vec_sq==-3)\n",
    "    \n",
    "    Nfixfix = Nfix - Nfixsacc - Nfixpliss\n",
    "    Nsaccsacc = Nsacc - Nsaccfix -Nsaccpliss\n",
    "    Nplisspliss = Npliss - Nplissfix - Nplisssacc\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    est_lambda_DB_list.append(-np.log((Nfixsacc +1) / Nfix))\n",
    "    est_lambda_DP_list.append(-np.log((Nfixpliss+1) / Nfix))\n",
    "    est_lambda_BD_list.append(-np.log((Nsaccfix +1)/ Nsacc))\n",
    "    est_lambda_BP_list.append(-np.log((Nsaccpliss+1) / Nsacc))\n",
    "    est_lambda_PD_list.append(-np.log((Nplissfix +1)/ Npliss))\n",
    "    est_lambda_PB_list.append(-np.log((Nplisssacc +1)/ Npliss))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2347ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gminima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(included)):\n",
    "    plt.hist(np.log(vS[i][bin_vec_vec[i]==0]+0.1),np.arange(-2,5,0.1),density=False);\n",
    "    plt.hist(np.log(vS[i][bin_vec_vec[i]==1]+0.1),np.arange(-2,5,0.1),density=False,alpha=0.6);\n",
    "    plt.hist(np.log(vS[i][bin_vec_vec[i]==2]+0.1),np.arange(-2,5,0.1),density=False,alpha=0.6);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(included)):\n",
    "    plt.hist(np.log(vS[i][bin_vec_vec[i]==2]+0.1),np.arange(-2,5,0.1),density=False,alpha=0.6);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff24fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "HMM_est_pliss_mu_b = []\n",
    "HMM_est_pliss_sigma_b = []\n",
    "HMM_est_sacc_mu_b = []\n",
    "HMM_est_sacc_sigma_b = []\n",
    "HMM_est_fix_mu_b = []\n",
    "HMM_est_fix_sigma_b = []\n",
    "HMM_est_D_list_b = []\n",
    "HMM_est_lambda_SF_list_b = []\n",
    "HMM_est_lambda_SP_list_b = []\n",
    "HMM_est_lambda_FS_list_b = []\n",
    "HMM_est_lambda_FP_list_b = []\n",
    "HMM_est_lambda_PS_list_b = []\n",
    "HMM_est_lambda_PF_list_b = []\n",
    "\n",
    "HMM_logdS_fix_list_b = []\n",
    "HMM_logdS_sacc_list_b = []\n",
    "HMM_logdS_pliss_list_b = []\n",
    "\n",
    "for i in range(len(included)):\n",
    "    print(i)\n",
    "    #############Pr Fixations ######################\n",
    "    prdb = np.exp(-est_lambda_DB_list[i])\n",
    "    prdp = np.exp(-est_lambda_DP_list[i])\n",
    "    prdd = 1-prdb - prdp\n",
    "    #############Pr Saccades########################\n",
    "    prbd = np.exp(-est_lambda_BD_list[i])\n",
    "    prbp = np.exp(-est_lambda_BP_list[i])\n",
    "    prbb = 1-prbd - prbp\n",
    "    #############Pr plissades ######################\n",
    "    prpd = np.exp(-est_lambda_PD_list[i])\n",
    "    prpb = np.exp(-est_lambda_PB_list[i])\n",
    "    prpp = 1-prpb - prpd\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    \n",
    "    s1 = State(LogNormalDistribution(est_fix_mu[i], est_fix_sigma[i])) #fixations\n",
    "    s2 = State(LogNormalDistribution(1.3*est_sacc_mu[i], est_sacc_sigma[i])) #saccades\n",
    "    s3 = State(LogNormalDistribution(1.3*est_pliss_mu[i], est_pliss_sigma[i])) #plissades\n",
    "    model = HiddenMarkovModel()\n",
    "    model.add_states(s1, s2, s3)\n",
    "    model.add_transition(model.start, s1, 0.4)\n",
    "    model.add_transition(model.start, s2, 0.3)\n",
    "    model.add_transition(model.start, s3, 0.3)\n",
    "    model.add_transition(s1, s1, prdd)\n",
    "    model.add_transition(s1, s2, prdb)\n",
    "    model.add_transition(s1, s3, prdp)\n",
    "    model.add_transition(s2, s1, prbd)\n",
    "    model.add_transition(s2, s2, prbb)\n",
    "    model.add_transition(s2, s3, prbp)\n",
    "    model.add_transition(s3, s1, prpd)\n",
    "    model.add_transition(s3, s2, prpb)\n",
    "    model.add_transition(s3, s3, prpp)\n",
    "    model.add_transition(s1, model.end, 0.4*len(vS[i][vS[i]>0]))\n",
    "    model.add_transition(s2, model.end, 0.3*len(vS[i][vS[i]>0]))\n",
    "    model.add_transition(s3, model.end, 0.3*len(vS[i][vS[i]>0]))\n",
    "    model.bake()\n",
    "    model.fit( [vS[i][vS[i]>0]] )\n",
    "    temp_vel = vS[i][vS[i]>0]\n",
    "    \n",
    "    s1_mu = model.states[0].distribution.parameters[0]\n",
    "    s2_mu = model.states[1].distribution.parameters[0]\n",
    "    s3_mu = model.states[2].distribution.parameters[0]\n",
    "    pos_array = np.argsort([s1_mu,s2_mu,s3_mu])\n",
    "    pos_array2 = np.array([np.where(np.argsort([s1_mu,s2_mu,s3_mu]) ==0)[0][0], np.where(np.argsort([s1_mu,s2_mu,s3_mu]) ==1)[0][0],np.where(np.argsort([s1_mu,s2_mu,s3_mu]) ==2)[0][0]])\n",
    " \n",
    "    HMM_est_fix_mu_b.append(model.states[pos_array[0]].distribution.parameters[0])\n",
    "    HMM_est_fix_sigma_b.append(model.states[pos_array[0]].distribution.parameters[1])\n",
    "    HMM_est_sacc_mu_b.append(model.states[pos_array[1]].distribution.parameters[0])\n",
    "    HMM_est_sacc_sigma_b.append(model.states[pos_array[1]].distribution.parameters[1])\n",
    "    HMM_est_pliss_mu_b.append(model.states[pos_array[2]].distribution.parameters[0])\n",
    "    HMM_est_pliss_sigma_b.append(model.states[pos_array[2]].distribution.parameters[1])\n",
    "    temp_HMM_bin_vec = np.swapaxes(model.viterbi(vS[i][vS[i]>0])[1],0,1)[0][1:-1]\n",
    "    HMM_bin_vec = np.array([pos_array2[kk] for kk in temp_HMM_bin_vec])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    #Nfix = len(HMM_bin_vec) - np.sum(HMM_bin_vec)\n",
    "    #Nsacc = np.sum(HMM_bin_vec)\n",
    "    #Ntransi = int(np.sum(np.abs(HMM_bin_vec[1:] - HMM_bin_vec[:-1]))/2)\n",
    "    #HMM_est_lambda_B_list.append(-np.log(1 - Ntransi / Nsacc))\n",
    "    #HMM_est_lambda_D_list.append(-np.log(1 - Ntransi / Nfix))\n",
    "    HMM_logdS_fix_list_b.append(temp_vel[HMM_bin_vec==0])\n",
    "    HMM_logdS_sacc_list_b.append(temp_vel[HMM_bin_vec==1])\n",
    "    HMM_logdS_pliss_list_b.append(temp_vel[HMM_bin_vec==2])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99606ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(included)):\n",
    "    plt.hist(np.log(HMM_logdS_fix_list_b[i]),np.arange(-2,3,0.2),density=False);\n",
    "    plt.hist(np.log(HMM_logdS_sacc_list_b[i]),np.arange(-2,3,0.2),density=False,alpha=0.6);\n",
    "    plt.hist(np.log(HMM_logdS_pliss_list_b[i]),np.arange(-2,3,0.2),density=False,alpha=0.4);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldetection, ldetectionfisher, lkmin, lfishermin = form_groups(vS_tot,np.arange(1,40,1)/max(vS_tot),True,'v','title','%.2f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "vS_tot = np.abs(np.hstack(vS) +0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2837f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(vS_tot),60);\n",
    "plt.vlines(2.7,0,300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(vS_tot[vS_tot>0]),100);\n",
    "plt.vlines(2.7,0,500000,color='r')\n",
    "plt.vlines(0.5,0,500000,color='k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0602525",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(vS_tot[vS_tot>2]),200);\n",
    "plt.vlines(2.7,0,5000,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca571a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvS = [vS_tot]\n",
    "\n",
    "\n",
    "est_pliss_mu = []\n",
    "est_pliss_sigma = []\n",
    "est_sacc_mu = []\n",
    "est_sacc_sigma = []\n",
    "est_fix_mu = []\n",
    "est_fix_sigma = []\n",
    "\n",
    "guess_logdS_fix_list = []\n",
    "guess_logdS_sacc_list = []\n",
    "guess_logdS_pliss_list = []\n",
    "est_lambda_DB_list= []\n",
    "est_lambda_DP_list= []\n",
    "est_lambda_BD_list= []\n",
    "est_lambda_BP_list= []\n",
    "est_lambda_PD_list= []\n",
    "est_lambda_PB_list= []\n",
    "bin_vec_vec = []\n",
    "for i in range(len(lvS)):\n",
    "    lthreshold = 3\n",
    "    lthreshold2 = 15\n",
    "    d__ss = lvS[i]\n",
    "    binary_vector1 = np.array([max(min(int(i-lthreshold+1),1),0) for i in d__ss])\n",
    "    binary_vector2 = np.array([max(min(int(i-lthreshold2+1),1),0) for i in d__ss])\n",
    "    binary_vector = binary_vector1 + binary_vector2\n",
    "    guess_dS_fix = lvS[i][binary_vector==0]\n",
    "    guess_dS_sacc = lvS[i][binary_vector==1]\n",
    "    guess_dS_pliss = lvS[i][binary_vector==2]\n",
    "    \n",
    "    guess_logdS_fix = np.log(guess_dS_fix[guess_dS_fix>0])\n",
    "    guess_logdS_fix_list.append(guess_logdS_fix)\n",
    "    guess_logdS_sacc = np.log(guess_dS_sacc[guess_dS_sacc>0])\n",
    "    guess_logdS_sacc_list.append(guess_logdS_sacc)\n",
    "    guess_logdS_pliss = np.log(guess_dS_pliss[guess_dS_pliss>0])\n",
    "    guess_logdS_pliss_list.append(guess_dS_pliss)\n",
    "    \n",
    "    est_fix_mu.append(np.mean(guess_logdS_fix))\n",
    "    est_fix_sigma.append(np.std(guess_logdS_fix))\n",
    "    est_sacc_mu.append(np.mean(guess_logdS_sacc))\n",
    "    est_sacc_sigma.append(np.std(guess_logdS_sacc))\n",
    "    est_pliss_mu.append(np.mean(guess_logdS_pliss))\n",
    "    est_pliss_sigma.append(np.std(guess_logdS_pliss))\n",
    "    bin_vec_vec.append(binary_vector)\n",
    "    \n",
    "    bin_vec_sq = binary_vector**2\n",
    "    diff_bin_vec_sq = binary_vector[1:]**2 - binary_vector[:-1]**2\n",
    "    \n",
    "    \n",
    "    Nfix = np.sum(binary_vector==0)\n",
    "    Nsacc = np.sum(binary_vector==1)\n",
    "    Npliss = np.sum(binary_vector==2)\n",
    "    \n",
    "    Nfixsacc =  np.sum(diff_bin_vec_sq==1)\n",
    "    Nfixpliss = np.sum(diff_bin_vec_sq==4)\n",
    "    Nsaccfix = np.sum(diff_bin_vec_sq==-1)\n",
    "    Nsaccpliss = np.sum(diff_bin_vec_sq==3)\n",
    "    Nplissfix = np.sum(diff_bin_vec_sq==-4)\n",
    "    Nplisssacc= np.sum(diff_bin_vec_sq==-3)\n",
    "    \n",
    "    Nfixfix = Nfix - Nfixsacc - Nfixpliss\n",
    "    Nsaccsacc = Nsacc - Nsaccfix -Nsaccpliss\n",
    "    Nplisspliss = Npliss - Nplissfix - Nplisssacc\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    est_lambda_DB_list.append(-np.log((Nfixsacc +1) / Nfix))\n",
    "    est_lambda_DP_list.append(-np.log((Nfixpliss+1) / Nfix))\n",
    "    est_lambda_BD_list.append(-np.log((Nsaccfix +1)/ Nsacc))\n",
    "    est_lambda_BP_list.append(-np.log((Nsaccpliss+1) / Nsacc))\n",
    "    est_lambda_PD_list.append(-np.log((Nplissfix +1)/ Npliss))\n",
    "    est_lambda_PB_list.append(-np.log((Nplisssacc +1)/ Npliss))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HMM_est_pliss_mu_b = []\n",
    "HMM_est_pliss_sigma_b = []\n",
    "HMM_est_sacc_mu_b = []\n",
    "HMM_est_sacc_sigma_b = []\n",
    "HMM_est_fix_mu_b = []\n",
    "HMM_est_fix_sigma_b = []\n",
    "HMM_est_D_list_b = []\n",
    "HMM_est_lambda_SF_list_b = []\n",
    "HMM_est_lambda_SP_list_b = []\n",
    "HMM_est_lambda_FS_list_b = []\n",
    "HMM_est_lambda_FP_list_b = []\n",
    "HMM_est_lambda_PS_list_b = []\n",
    "HMM_est_lambda_PF_list_b = []\n",
    "\n",
    "HMM_logdS_fix_list_b = []\n",
    "HMM_logdS_sacc_list_b = []\n",
    "HMM_logdS_pliss_list_b = []\n",
    "\n",
    "for i in range(len(lvS)):\n",
    "    print(i)\n",
    "    #############Pr Fixations ######################\n",
    "    prdb = 0.2*np.exp(-est_lambda_DB_list[i])\n",
    "    prdp = 0.2*np.exp(-est_lambda_DP_list[i])\n",
    "    prdd = 1-prdb - prdp\n",
    "    #############Pr Saccades########################\n",
    "    prbd = 0.2*np.exp(-est_lambda_BD_list[i])\n",
    "    prbp = 0.2*np.exp(-est_lambda_BP_list[i])\n",
    "    prbb = 1-prbd - prbp\n",
    "    #############Pr plissades ######################\n",
    "    prpd = 0.2*np.exp(-est_lambda_PD_list[i])\n",
    "    prpb = 0.2*np.exp(-est_lambda_PB_list[i])\n",
    "    prpp = 1-prpb - prpd\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    \n",
    "    s1 = State(LogNormalDistribution(est_fix_mu[i], est_fix_sigma[i])) #fixations\n",
    "    s2 = State(LogNormalDistribution(1.3*est_sacc_mu[i], est_sacc_sigma[i])) #saccades\n",
    "    s3 = State(LogNormalDistribution(1.3*est_pliss_mu[i], est_pliss_sigma[i])) #plissades\n",
    "    model = HiddenMarkovModel()\n",
    "    model.add_states(s1, s2, s3)\n",
    "    model.add_transition(model.start, s1, 0.4)\n",
    "    model.add_transition(model.start, s2, 0.3)\n",
    "    model.add_transition(model.start, s3, 0.3)\n",
    "    model.add_transition(s1, s1, prdd)\n",
    "    model.add_transition(s1, s2, prdb)\n",
    "    model.add_transition(s1, s3, prdp)\n",
    "    model.add_transition(s2, s1, prbd)\n",
    "    model.add_transition(s2, s2, prbb)\n",
    "    model.add_transition(s2, s3, prbp)\n",
    "    model.add_transition(s3, s1, prpd)\n",
    "    model.add_transition(s3, s2, prpb)\n",
    "    model.add_transition(s3, s3, prpp)\n",
    "    model.add_transition(s1, model.end, 0.4*len(lvS[i][lvS[i]>0]))\n",
    "    model.add_transition(s2, model.end, 0.3*len(lvS[i][lvS[i]>0]))\n",
    "    model.add_transition(s3, model.end, 0.3*len(lvS[i][lvS[i]>0]))\n",
    "    model.bake()\n",
    "    model.fit( [lvS[i][lvS[i]>0]] )\n",
    "    temp_vel = lvS[i][lvS[i]>0]\n",
    "    \n",
    "    s1_mu = model.states[0].distribution.parameters[0]\n",
    "    s2_mu = model.states[1].distribution.parameters[0]\n",
    "    s3_mu = model.states[2].distribution.parameters[0]\n",
    "    pos_array = np.argsort([s1_mu,s2_mu,s3_mu])\n",
    "    pos_array2 = np.array([np.where(np.argsort([s1_mu,s2_mu,s3_mu]) ==0)[0][0], np.where(np.argsort([s1_mu,s2_mu,s3_mu]) ==1)[0][0],np.where(np.argsort([s1_mu,s2_mu,s3_mu]) ==2)[0][0]])\n",
    " \n",
    "    HMM_est_fix_mu_b.append(model.states[pos_array[0]].distribution.parameters[0])\n",
    "    HMM_est_fix_sigma_b.append(model.states[pos_array[0]].distribution.parameters[1])\n",
    "    HMM_est_sacc_mu_b.append(model.states[pos_array[1]].distribution.parameters[0])\n",
    "    HMM_est_sacc_sigma_b.append(model.states[pos_array[1]].distribution.parameters[1])\n",
    "    HMM_est_pliss_mu_b.append(model.states[pos_array[2]].distribution.parameters[0])\n",
    "    HMM_est_pliss_sigma_b.append(model.states[pos_array[2]].distribution.parameters[1])\n",
    "    temp_HMM_bin_vec = np.swapaxes(model.viterbi(lvS[i][lvS[i]>0])[1],0,1)[0][1:-1]\n",
    "    HMM_bin_vec = np.array([pos_array2[kk] for kk in temp_HMM_bin_vec])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    #Nfix = len(HMM_bin_vec) - np.sum(HMM_bin_vec)\n",
    "    #Nsacc = np.sum(HMM_bin_vec)\n",
    "    #Ntransi = int(np.sum(np.abs(HMM_bin_vec[1:] - HMM_bin_vec[:-1]))/2)\n",
    "    #HMM_est_lambda_B_list.append(-np.log(1 - Ntransi / Nsacc))\n",
    "    #HMM_est_lambda_D_list.append(-np.log(1 - Ntransi / Nfix))\n",
    "    HMM_logdS_fix_list_b.append(temp_vel[HMM_bin_vec==0])\n",
    "    HMM_logdS_sacc_list_b.append(temp_vel[HMM_bin_vec==1])\n",
    "    HMM_logdS_pliss_list_b.append(temp_vel[HMM_bin_vec==2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da688670",
   "metadata": {},
   "outputs": [],
   "source": [
    "    HMM_logdS_fix_list_b.append(temp_vel[HMM_bin_vec==0])\n",
    "    HMM_logdS_sacc_list_b.append(temp_vel[HMM_bin_vec==1])\n",
    "    HMM_logdS_pliss_list_b.append(temp_vel[HMM_bin_vec==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9573b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.hstack(vS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    temp_HMM_bin_vec = np.swapaxes(model.viterbi(lvS[i][lvS[i]>0])[1],0,1)[0][1:-1]\n",
    "    HMM_bin_vec = np.array([pos_array2[kk] for kk in temp_HMM_bin_vec])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    #Nfix = len(HMM_bin_vec) - np.sum(HMM_bin_vec)\n",
    "    #Nsacc = np.sum(HMM_bin_vec)\n",
    "    #Ntransi = int(np.sum(np.abs(HMM_bin_vec[1:] - HMM_bin_vec[:-1]))/2)\n",
    "    #HMM_est_lambda_B_list.append(-np.log(1 - Ntransi / Nsacc))\n",
    "    #HMM_est_lambda_D_list.append(-np.log(1 - Ntransi / Nfix))\n",
    "    HMM_logdS_fix_list_b.append(temp_vel[HMM_bin_vec==0])\n",
    "    HMM_logdS_sacc_list_b.append(temp_vel[HMM_bin_vec==1])\n",
    "    HMM_logdS_pliss_list_b.append(temp_vel[HMM_bin_vec==2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "plt.hist(np.log(HMM_logdS_fix_list_b[i]),np.arange(-2,4,0.2),density=False);\n",
    "plt.hist(np.log(HMM_logdS_sacc_list_b[i]),np.arange(-2,4,0.2),density=False,alpha=0.6);\n",
    "plt.hist(np.log(HMM_logdS_pliss_list_b[i]),np.arange(-2,4,0.2),density=False,alpha=0.4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792b6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b711a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314a37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809de3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52dae8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc0703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c87653c5",
   "metadata": {},
   "source": [
    "import scipy.signal\n",
    "N=5\n",
    "min_first_min = 10 #first minimum should be lower than this number.\n",
    "vale_ratio = 1.2\n",
    "#for i in range(len(detection_list)):\n",
    "for i in range(1):\n",
    "    lc_smooth_det = np.array(np.convolve(detection_list[i], np.ones(N), 'valid') / N)\n",
    "    #lc_smooth_det = np.array(detection_list[i])\n",
    "    plt.plot(lc_smooth_det)\n",
    "    temp_minima = scipy.signal.argrelextrema(lc_smooth_det, np.less)[0]\n",
    "    minima = []\n",
    "    for k in range(len(temp_minima)):\n",
    "        ####################\n",
    "        if k ==0:\n",
    "            if temp_minima[0]>min_first_min:\n",
    "                minima.append(0)\n",
    "            else: \n",
    "                minima.append(temp_minima[0])\n",
    "        #####################\n",
    "        if k == len(temp_minima)-1:\n",
    "            if temp_minima[k]< vale_ratio*max(lc_smooth_det[temp_minima[k]:]):\n",
    "                if temp_minima[k]< vale_ratio*max(lc_smooth_det[temp_minima[k-1]:temp_minima[i]]):\n",
    "                    minima.append(temp_minima[k])\n",
    "        else:\n",
    "            if temp_minima[k]< vale_ratio*max(lc_smooth_det[temp_minima[k]:temp_minima[k+1]]):\n",
    "                if temp_minima[k]< vale_ratio**max(lc_smooth_det[temp_minima[k-1]:temp_minima[k]]):\n",
    "                    minima.append(temp_minima[k])\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "    plt.vlines(minima,0,0.3,color='k')\n",
    "    plt.vlines(temp_minima,0,0.3,color='dimgrey')\n",
    "    print(scipy.signal.argrelextrema(lc_smooth_det, np.less))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60ef34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f209775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "N=5\n",
    "for i in range(len(detection_list)):\n",
    "    lc_smooth_det = np.convolve(detection_list[i], np.ones(N), 'valid') / N\n",
    "    plt.plot(lc_smooth_det)\n",
    "    #minima = scipy.signal.argrelextrema(lc_smooth_det, np.less)\n",
    "    minima = scipy.signal.find_peaks(lc_smooth_det, distance=14)[0]\n",
    "    plt.vlines(minima,0.1,0.3,color='k')\n",
    "    print(scipy.signal.argrelextrema(lc_smooth_det, np.less))\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_minima"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5815aedf",
   "metadata": {},
   "source": [
    "import scipy.signal\n",
    "N=3\n",
    "min_first_min = 10 #first minimum should be lower than this number.\n",
    "vale_ratio = 1.2\n",
    "#for i in range(len(detection_list)):\n",
    "for i in range(1):\n",
    "    lc_smooth_det = np.array(np.convolve(detection_list[i], np.ones(N), 'valid') / N)\n",
    "    #lc_smooth_det = np.array(detection_list[i])\n",
    "    \n",
    "    temp_minima = scipy.signal.argrelextrema(lc_smooth_det, np.less)[0]\n",
    "    temp_maxima = scipy.signal.argrelextrema(lc_smooth_det,np.greater)[0]\n",
    "    if lc_smooth_det[-1] > lc_smooth_det[-2]:\n",
    "        np.array(list(temp_maxima).append(len(lc_smooth_det)-1))\n",
    "    minima = []\n",
    "    ####################\n",
    "    if temp_minima[0]>min_first_min:\n",
    "        minima.append(0)\n",
    "    else: \n",
    "        minima.append(temp_minima[0])\n",
    "    #####################\n",
    "    for k in range(len(temp_maxima)-1):\n",
    "        valey_min = min(lc_smooth_det[temp_maxima[k]:temp_maxima[k+1]])\n",
    "        \n",
    "        if valey_min < vale_ratio*lc_smooth_det[temp_maxima[k]]:\n",
    "            if valey_min < vale_ratio*lc_smooth_det[temp_maxima[k+1]]:\n",
    "                minima.append(np.argmin(lc_smooth_det[temp_maxima[k]:temp_maxima[k+1]]))\n",
    "                        \n",
    "\n",
    "            \n",
    "    plt.plot(lc_smooth_det)        \n",
    "    plt.vlines(minima,0,0.3,color='k')\n",
    "    print(scipy.signal.argrelextrema(lc_smooth_det, np.less))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.signal.argrelextrema(lc_smooth_det,np.greater)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_smooth_det[28:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e93fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lc_smooth_det)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c311204",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_minima[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a752c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6316f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "minima[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6fd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdd6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91618a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4411b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81328fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dae7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1c008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787952b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab18916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c0d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71709c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b29937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0020de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31825108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390e851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list = np.power(2,np.arange(9))\n",
    "len_vector = len(dS2_lag[0])\n",
    "adj_r_square_lev = []\n",
    "adj_r_square_int = []\n",
    "params_int = []\n",
    "params_lev = []\n",
    "\n",
    "for part in range(0,len(participants)):\n",
    "    initial_conditions = []\n",
    "    for k in range(len_vector-2):\n",
    "        blabla = first_estimate_ple_averages_inc(dS2_lag[part][k:k+3],tau_list)\n",
    "        if blabla[1]:\n",
    "            initial_conditions.append(blabla[0])\n",
    "\n",
    "\n",
    "    if len(initial_conditions)!=0:\n",
    "\n",
    "        best_params = optimize(dS2_lag[part],tau_list,initial_conditions[0],1000,50)\n",
    "        curr_min = np.mean( (np.log2(dS2_lag[part]) - log2_moment_scaling(tau_list,best_params[0],best_params[1] ,best_params[2]))**2)\n",
    "        best_params_index = 0\n",
    "        opt_meu = 1\n",
    "        for k in range(len(initial_conditions)):\n",
    "            popt3 = optimize(dS2_lag[part],tau_list,initial_conditions[k],1000,50)\n",
    "            new_min = np.mean((np.log2(dS2_lag[part]) - log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2]))**2 )\n",
    "            if new_min != new_min:\n",
    "                new_min = curr_min + 1000\n",
    "            if new_min < curr_min:\n",
    "                best_params = popt3\n",
    "                curr_min = new_min\n",
    "                best_params_index = k\n",
    "                opt_meu = 1\n",
    "\n",
    "            try:\n",
    "                popt3, pcov = scipy.optimize.curve_fit(log2_moment_scaling, tau_list, np.log2(dS2_lag[part]),p0=initial_conditions[k], maxfev=500000)\n",
    "                new_min = np.mean((np.log2(dS2_lag[part]) - log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2]))**2 )\n",
    "                if new_min != new_min:\n",
    "                    new_min = curr_min + 1000\n",
    "                if new_min < curr_min:\n",
    "                    best_params = popt3\n",
    "                    curr_min = new_min\n",
    "                    best_params_index = k\n",
    "                    opt_meu = 0\n",
    "            except RuntimeError:\n",
    "                'a'\n",
    "        print(best_params_index,opt_meu)\n",
    "\n",
    "\n",
    "    if len(initial_conditions)==0:\n",
    "        best_params = optimize(dS2_lag[part],tau_list,[-1,1,0.01],1000,20)\n",
    "        curr_min = np.mean( (np.log2(dS2_lag[part]) - log2_moment_scaling(tau_list,best_params[0],best_params[1] ,best_params[2]))**2)\n",
    "        best_params_index = -1\n",
    "        opt_meu = 1\n",
    "\n",
    "        try:\n",
    "            popt3, pcov = scipy.optimize.curve_fit(log2_moment_scaling, tau_list, np.log2(dS2_lag[part]),p0=[-1,1,0.01], maxfev=50000000)\n",
    "            new_min = np.mean((np.log2(dS2_lag[part]) - log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2]))**2 )\n",
    "\n",
    "            if new_min != new_min:\n",
    "                new_min = curr_min + 1000\n",
    "            if new_min < curr_min:\n",
    "                best_params = popt3\n",
    "                curr_min = new_min\n",
    "                best_params_index = k\n",
    "                opt_meu = 0\n",
    "\n",
    "        except RuntimeError:\n",
    "            'a'\n",
    "\n",
    "    print(best_params_index,opt_meu)\n",
    "    \n",
    "    params_int.append(best_params)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    reg2 = LinearRegression().fit(np.log2(np.array(tau_list)).reshape(-1, 1), np.log2(dS2_lag[part]))\n",
    "    coef = reg2.coef_[0]\n",
    "    intercept = reg2.intercept_\n",
    "    \n",
    "    params_lev.append([intercept,coef])\n",
    "    \n",
    "    adj_r_square_lev.append(adjusted_r_square(np.log2(dS2_lag[part]),np.arange(7)*coef + intercept,2))\n",
    "    adj_r_square_int.append(adjusted_r_square(np.log2(dS2_lag[part]),log2_moment_scaling(tau_list,best_params[0],best_params[1] ,best_params[2]),3))\n",
    "#popt2, pcov2 = scipy.optimize.curve_fit(exponential_fit, np.array(tau_list), np.array(ds_square_lag), maxfev=5000000)\n",
    "#print(popt2)\n",
    "#print(C1_new2(tau,g_alpha,g_beta,g_D,g_v),C2_new2(tau,g_alpha,g_beta,g_D,g_v),lambda2)\n",
    "#plt.plot(Sergyi_expr_simp(np.arange(tau_list),*popt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dS2_lag[part]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(adj_r_square_int) - np.array(  adj_r_square_lev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a026d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(np.log10(np.array(adj_r_square_lev)[np.array(adj_r_square_int)>0.7]  / np.array(adj_r_square_int)[np.array(adj_r_square_int)>0.7]), np.arange(-0.1,0.1,0.01),color='dimgrey')\n",
    "plt.hist(np.array(adj_r_square_int) - np.array(adj_r_square_lev), np.arange(-0.1,0.1,0.01),color='dimgrey')\n",
    "plt.vlines(0, 0, 30,colors='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(np.array(adj_r_square_lev)  / np.array(adj_r_square_int) ), np.arange(-1,1.1,0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d021fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log2_moment_scaling(tau_list,initial_conditions[0],initial_conditions[1] ,initial_conditions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log2(dS2_lag[part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6bb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r_square_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bca605",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r_square_lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log2(dS2_lag[part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_r_square(np.log2(dS2_lag[part]),log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b111012",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r_square_lev = []\n",
    "adj_r_square_int = []\n",
    "for i in range(60):\n",
    "    print(i)\n",
    "    if i !=33:\n",
    "        tau_list = np.power(2,np.arange(7))\n",
    "        popt3, pcov = scipy.optimize.curve_fit(log2_moment_scaling, tau_list, np.log2(dS2_lag[i]), maxfev=5000000)\n",
    "        print(popt3)\n",
    "\n",
    "        reg2 = LinearRegression().fit(np.log2(np.array(tau_list)).reshape(-1, 1), np.log2(dS2_lag[i]))\n",
    "        coef = reg2.coef_[0]\n",
    "        intercept = reg2.intercept_\n",
    "\n",
    "        adj_r_square_lev.append(adjusted_r_square(np.log2(dS2_lag[i]),np.arange(7)*coef + intercept,2))\n",
    "        adj_r_square_int.append(adjusted_r_square(np.log2(dS2_lag[i]),log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bb561",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.log10(np.array(adj_r_square_lev)/ np.array(adj_r_square_int))\n",
    "plt.hist(gamma,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c13491",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r_square_lev_test = []\n",
    "adj_r_square_int_test = []\n",
    "adj_r_square_lev_control = []\n",
    "adj_r_square_int_control = []\n",
    "for i in range(60):\n",
    "    print(i)\n",
    "    if i !=33:\n",
    "        if i%2 == 1:\n",
    "            tau_list = np.power(2,np.arange(7))\n",
    "            popt3, pcov = scipy.optimize.curve_fit(log2_moment_scaling, tau_list, np.log2(dS2_lag[i]), maxfev=5000000)\n",
    "            print(popt3)\n",
    "\n",
    "            reg2 = LinearRegression().fit(np.log2(np.array(tau_list)).reshape(-1, 1), np.log2(dS2_lag[i]))\n",
    "            coef = reg2.coef_[0]\n",
    "            intercept = reg2.intercept_\n",
    "\n",
    "            adj_r_square_lev_control.append(adjusted_r_square(np.log2(dS2_lag[i]),np.arange(7)*coef + intercept,2))\n",
    "            adj_r_square_int_control.append(adjusted_r_square(np.log2(dS2_lag[i]),log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2]),3))\n",
    "        \n",
    "        \n",
    "        if i%2==0:\n",
    "            tau_list = np.power(2,np.arange(7))\n",
    "            popt3, pcov = scipy.optimize.curve_fit(log2_moment_scaling, tau_list, np.log2(dS2_lag[i]), maxfev=5000000)\n",
    "            print(popt3)\n",
    "\n",
    "            reg2 = LinearRegression().fit(np.log2(np.array(tau_list)).reshape(-1, 1), np.log2(dS2_lag[i]))\n",
    "            coef = reg2.coef_[0]\n",
    "            intercept = reg2.intercept_\n",
    "\n",
    "            adj_r_square_lev_test.append(adjusted_r_square(np.log2(dS2_lag[i]),np.arange(7)*coef + intercept,2))\n",
    "            adj_r_square_int_test.append(adjusted_r_square(np.log2(dS2_lag[i]),log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2]),3))\n",
    "\n",
    "gamma_test = np.log10(np.array(adj_r_square_lev_test)/ np.array(adj_r_square_int_test))\n",
    "gamma_control = np.log10(np.array(adj_r_square_lev_control)/ np.array(adj_r_square_int_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23148719",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gamma_test,np.arange(-0.05,0.05,0.0025),alpha=0.5)\n",
    "plt.hist(gamma_control,np.arange(-0.05,0.05,0.0025),alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501eec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(gamma_test>=0))\n",
    "print(np.sum(gamma_control>=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83aef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(gamma_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc82a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r_square_lev = []\n",
    "adj_r_square_int = []\n",
    "for i in range(60):\n",
    "    print(i)\n",
    "    if i !=33:\n",
    "        tau_list = np.power(2,np.arange(7))\n",
    "        popt3, pcov = scipy.optimize.curve_fit(log2_moment_scaling, tau_list, np.log2(dS2_lag[i]), maxfev=5000000)\n",
    "        print(popt3)\n",
    "\n",
    "        reg2 = LinearRegression().fit(np.log2(np.array(tau_list)).reshape(-1, 1), np.log2(dS2_lag[i]))\n",
    "        coef = reg2.coef_[0]\n",
    "        intercept = reg2.intercept_\n",
    "\n",
    "        adj_r_square_lev.append(adjusted_r_square(np.log2(dS2_lag[i]),np.arange(7)*coef + intercept,2))\n",
    "        adj_r_square_int.append(adjusted_r_square(np.log2(dS2_lag[i]),log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2]),3))\n",
    "\n",
    "        plt.plot(np.log2(dS2_lag[i]),'ks',label='eye gaze trajectory',alpha=0.5)\n",
    "        plt.plot(log2_moment_scaling(tau_list,popt3[0],popt3[1] ,popt3[2])  ,label='intermittent fit',c='tab:blue')\n",
    "        plt.plot(np.arange(7)*coef + intercept  ,label='Lévy walk fit',c='tab:red')\n",
    "        plt.xlabel(r'$log_2(\\tau)$')\n",
    "        plt.ylabel(r'$log \\|\\|u^2\\|\\|$')\n",
    "        plt.legend()\n",
    "        plt.savefig('Fig4-test_old_data_P' + str(i) + '.png',dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "#plt.plot(np.log2(moment_scaling(np.array(tau_list),popt[0],popt[1],popt[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DATA IN INDEPENDENT SERIES. \n",
    "#THE ORIGINAL DATA DOES NOT SEPARATE BETWEEN TRIALS OF EACH PARTICIPANT AND INCLUES DE DATA-POINTS OF THE CALLIBRATION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = rmissingvaluecol(data[0],100)\n",
    "NaN_percentage = []\n",
    "all_series = []\n",
    "for i in range(len(data)):\n",
    "    if i % 2 == 0:\n",
    "        group = 'dataset_III'\n",
    "    if i % 2 == 1:\n",
    "        group = 'dataset_II'\n",
    "    cal_end = data[i].loc[data[i]['Event'] == 'Eye tracker Calibration end'].index #END OF CALLIBRATION\n",
    "    rec_end = data[i].loc[data[i]['Event'] == 'RecordingEnd'].index #END OF TRIAL\n",
    "    count = 0\n",
    "    for j in range(len(cal_end)):\n",
    "        lnew_series = data[i].iloc[cal_end[j]: rec_end[j+count]]\n",
    "        new_series = lnew_series[selected_rows].copy()  #TIME-SERIES OF ONE INDIVIDUAL TRIAL\n",
    "        length = rec_end[j+count] - cal_end[j]\n",
    "        while length<0:\n",
    "            count +=1\n",
    "            new_series = data[i].iloc[cal_end[j]: rec_end[j+count]]\n",
    "            length = rec_end[j+count] - cal_end[j]\n",
    "            \n",
    "##################################################################\n",
    "        if length > 600: #WE WILL NOT CONSIDER TRIALS WITH LESS THAN 600 POINTS\n",
    "            im_name =  which_image(data[i]['Presented Stimulus name'].iloc[cal_end[j]: rec_end[j+count]])\n",
    "            nan_rat = 1- new_series['Gaze point X'].dropna().shape[0] / new_series['Gaze point X'].shape[0]\n",
    "            if nan_rat < 0.6:\n",
    "                #new_series.dropna(how='all', axis=1).to_csv('EyeT_group_'+group+ '_image_name_' + im_name +'_'+'participant_'+str(i)+'_'+'trial'+str(j)+'.csv')\n",
    "\n",
    "                all_series.append(new_series) #LIST OF TIME-SERIES OF TRIALS\n",
    "                NaN_percentage.append(new_series['Gaze point X'].dropna().shape[0] / new_series['Gaze point X'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SERIES LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for j in range(len(all_series)):\n",
    "    lengths.append(len(all_series[j]['Gaze point X'].dropna()))\n",
    "#plt.hist(lengths,np.arange(4000,12000,500))\n",
    "a = plt.hist(lengths,np.arange(0,30000,1200))\n",
    "np.savetxt(\"hist_length_time_series.txt\",np.transpose([np.arange(600,30000-600,1200),np.array(a[0])]))\n",
    "plt.xlabel('data-points in each time-series')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('histogram: length of each time-series')\n",
    "plt.savefig('data_points_frequency.png',dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ffe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PERCENTAGE OF MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fc445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = plt.hist(np.ones(len(NaN_percentage))- np.array(NaN_percentage),np.arange(0,0.6,0.025))\n",
    "np.savetxt(\"NaN_ratio_frequencies_x_axis.txt\",a[1])\n",
    "np.savetxt(\"NaN_ratio_frequencies.txt\",np.transpose([np.arange(0.0125,0.6-0.0125,0.025),np.array(a[0])/np.sum(np.array(a[0]))]))\n",
    "plt.title('Histogram of NaN ratios')\n",
    "plt.xlabel('NaN ratio')\n",
    "plt.ylabel('Frequency in our database')\n",
    "plt.savefig('hist_NaN',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadcb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_all_series= []\n",
    "for j in range(len(all_series)):\n",
    "    new_array = []\n",
    "    current_array = np.array(all_series[j].dropna(subset=['Gaze point X'])['Eye movement type'])\n",
    "    for i in range(len(current_array)):\n",
    "        if current_array[i] == 'Saccade':\n",
    "            new_array.append(1)\n",
    "        elif current_array[i] == 'Fixation':\n",
    "            new_array.append(0)\n",
    "        elif current_array[i]  == 'Unclassified':\n",
    "            new_array.append(-1)\n",
    "        elif current_array[i] == 'EyesNotFound':\n",
    "            new_array.append(-2)\n",
    "    durations_all_series.append(new_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING THE LABELS OF SACCADE AND FIXATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fix = 0\n",
    "total_sac = 0\n",
    "total_unclassified = 0\n",
    "for j in range(len(all_series)):\n",
    "    current_array = np.array(all_series[j].dropna(subset=['Gaze point X'])['Eye movement type'])\n",
    "    for i in range(len(all_series[j].dropna(subset=['Gaze point X'])['Eye movement type'])):\n",
    "        if current_array[i] == 'Saccade':\n",
    "            total_sac +=1\n",
    "        elif current_array[i] == 'Fixation':\n",
    "            total_fix +=1\n",
    "        elif current_array[i] == 'Unclassified':\n",
    "            total_unclassified +=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_fix)\n",
    "print(total_sac)\n",
    "print(total_unclassified)\n",
    "print(total_fix+total_sac+total_unclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSECUTIVE FIXATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cons_fix = []\n",
    "total_cons_sacc = []\n",
    "for j in range(len(all_series)-1):\n",
    "    consecutive_fix = consecutive_sac(durations_all_series[j] , 0)  \n",
    "    total_cons_fix = total_cons_fix + list(consecutive_fix[0])\n",
    "    consecutive_sacc = consecutive_sac(durations_all_series[j],1)  \n",
    "    total_cons_sac = total_cons_sac + list(consecutive_sacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3bbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "total_cons_fix = []\n",
    "for j in range(len(all_series)-1):\n",
    "    consecutive_fix = consecutive_sac(durations_all_series[j],0)  \n",
    "    total_cons_fix = total_cons_fix + list(consecutive_fix[0])\n",
    "    band = 1.06*np.std(consecutive_fix[0])*(len(consecutive_fix[0]))**(-0.2)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=band).fit(np.array(consecutive_fix[0]).reshape(-1, 1))\n",
    "    log_dens = np.exp(kde.score_samples(np.arange(0,200).reshape(-1, 1)))\n",
    "    plt.plot(log_dens)\n",
    "    plt.xlabel('consecutive fixations')\n",
    "    plt.ylabel('probability density')\n",
    "    means.append(np.mean(consecutive_fix[0]))\n",
    "plt.savefig('fix_duration.png',dpi = 500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b15910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(kernel='gaussian', bandwidth=band).fit(np.array(total_cons_fix).reshape(-1, 1))\n",
    "log_dens_fix = np.exp(kde.score_samples(np.arange(0,200).reshape(-1, 1)))\n",
    "a = np.transpose([np.arange(0,200),log_dens_fix])\n",
    "#np.savetxt(\"total_fixations_length_pdf_x_axis\"+\".txt\",np.arange(200))\n",
    "plt.plot(log_dens)\n",
    "plt.xlabel('consecutive fixations')\n",
    "plt.ylabel('probability density')\n",
    "plt.title('Total Fixation Length pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc825872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSECUTIVE SACCADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81317aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cons_sac = []\n",
    "for j in range(len(all_series)):\n",
    "    consecutive_fix = consecutive_sac(durations_all_series[j],1)   \n",
    "    total_cons_sac = total_cons_sac + list(consecutive_fix[0])\n",
    "    band = 2*1.06*np.std(consecutive_fix[0])*(len(consecutive_fix[0]))**(-0.2)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=band).fit(np.array(consecutive_fix[0]).reshape(-1, 1))\n",
    "    log_dens = np.exp(kde.score_samples(np.arange(0,20).reshape(-1, 1)))\n",
    "    plt.plot(np.arange(0,20),log_dens)\n",
    "    plt.xlabel('consecutive saccades')\n",
    "    plt.ylabel('probability density')\n",
    "    np.savetxt(\"saccades_length_pdf\"+str(j) + \".txt\",log_dens)\n",
    "    np.savetxt(\"saccades_length_x_axis\"+str(j) + \".txt\",np.arange(0,20))\n",
    "    means.append(np.mean(consecutive_fix[0]))\n",
    "plt.savefig('sac_duration.png',dpi = 500)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(kernel='gaussian', bandwidth=band).fit(np.array(total_cons_sac).reshape(-1, 1))\n",
    "log_dens_sac = np.exp(kde.score_samples(np.arange(0,20).reshape(-1, 1)))\n",
    "bb = np.transpose([np.arange(1,21),log_dens_sac])\n",
    "np.savetxt(\"total_sac_length_pdf\"+ \".txt\",bb)\n",
    "plt.plot(log_dens)\n",
    "plt.xlabel('consecutive saccades')\n",
    "plt.ylabel('probability density')\n",
    "plt.title('Total Saccade Length pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio of Saccades and Fixations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_sacc_array= []\n",
    "percentage_sacc = []\n",
    "\n",
    "for j in range(len(all_series)):\n",
    "    a = consecutive_sac(durations_all_series[j],1)\n",
    "    consecutive_sacc_array.append(a[0]) \n",
    "    percentage_sacc.append(np.array(a[0]).sum()/ (all_series[j]['Gaze point X'].dropna().shape[0]))\n",
    "    \n",
    "a = plt.hist(percentage_sacc,np.arange(0,1.02,0.02))\n",
    "plt.xlabel('saccade to series length ratio')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('')\n",
    "plt.savefig('./histogram saccade-ratio.png',dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_fix_array= []\n",
    "percentage_fix = []\n",
    "\n",
    "for j in range(len(all_series)):\n",
    "    a = consecutive_sac(durations_all_series[j],0)\n",
    "    consecutive_fix_array.append(a[0]) \n",
    "    percentage_fix.append(np.array(a[0]).sum()/ (all_series[j]['Gaze point X'].dropna().shape[0]))\n",
    "    \n",
    "a = plt.hist(percentage_fix,np.arange(0,1,0.04))\n",
    "np.savetxt(\"percentage_fix.txt\",np.transpose([np.arange(0.02,0.98,0.04),np.array(a[0])/np.sum(np.array(a[0]))]))\n",
    "plt.xlabel('fixation to series length ratio')\n",
    "plt.ylabel('frequency')\n",
    "#plt.title('histogram: saccade-ratio')\n",
    "plt.savefig('./histogram fixation-ratio.png',dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49078451",
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_unclassified_array= []\n",
    "percentage_unclassified = []\n",
    "\n",
    "for j in range(len(all_series)):\n",
    "    a = consecutive_sac(durations_all_series[j],-1)\n",
    "    consecutive_unclassified_array.append(a[0]) \n",
    "    percentage_unclassified.append(np.array(a[0]).sum()/ (all_series[j]['Gaze point X'].dropna().shape[0]))\n",
    "    \n",
    "a = plt.hist(percentage_unclassified,np.arange(0,1,0.04))\n",
    "np.savetxt(\"percentage_unclassified.txt\",np.transpose([np.arange(0.02,0.98,0.04),np.array(a[0])/np.sum(np.array(a[0]))]))\n",
    "plt.xlabel(\" 'Unclassified'/'Eyes not found' to series length ratio\")\n",
    "plt.ylabel('frequency')\n",
    "plt.savefig('./histogram unclassified-ratio.png',dpi = 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
